2025-06-09 15:51:37,720 - __main__ - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:51:37,720 - __main__ - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:51:37,720 - __main__ - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:51:37,744 - __main__ - INFO - [<module>:1429] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-09 15:51:37,770 - cogview4_api_server - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:51:37,770 - cogview4_api_server - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:51:37,770 - cogview4_api_server - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:51:37,875 - cogview4_api_server - INFO - [lifespan:1155] - Starting CogView4 API server...
2025-06-09 15:51:37,875 - cogview4_api_server - INFO - [__init__:871] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-09 15:51:41,674 - __mp_main__ - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:51:41,674 - __mp_main__ - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:51:41,674 - __mp_main__ - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:51:41,693 - cogview4_api_server - INFO - [__init__:895] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-09 15:51:41,694 - cogview4_api_server - INFO - [__init__:908] - Started worker process 0 using spawn method
2025-06-09 15:51:41,695 - cogview4_api_server - INFO - [__init__:908] - Started worker process 1 using spawn method
2025-06-09 15:51:41,695 - cogview4_api_server - INFO - [__init__:910] - Worker pool initialized with 2 workers
2025-06-09 15:51:41,696 - cogview4_api_server - INFO - [__init__:920] - Prompt batching enabled - timeout checker started
2025-06-09 15:51:41,696 - cogview4_api_server - INFO - [lifespan:1158] - Worker pool initialized successfully!
2025-06-09 15:51:45,544 - __mp_main__ - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:51:45,544 - __mp_main__ - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:51:45,544 - __mp_main__ - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:51:45,544 - __mp_main__ - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:51:45,544 - __mp_main__ - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:51:45,544 - __mp_main__ - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:51:45,554 - cogview4_api_server - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:51:45,554 - cogview4_api_server - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:51:45,554 - cogview4_api_server - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:51:45,554 - cogview4_api_server - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:51:45,554 - cogview4_api_server - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:51:45,554 - cogview4_api_server - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:51:45,565 - worker_1 - INFO - [worker_process:174] - Worker 1 starting up...
2025-06-09 15:51:45,565 - worker_0 - INFO - [worker_process:174] - Worker 0 starting up...
2025-06-09 15:51:45,565 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-09 15:51:45,565 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-09 15:51:45,565 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-09 15:51:45,689 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-09 15:51:45,689 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-09 15:51:48,568 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-09 15:51:48,717 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-09 15:51:48,717 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-09 15:52:19,586 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-09 15:52:19,588 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-09 15:52:19,588 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-09 15:52:24,889 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-09 15:52:24,891 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-09 15:52:24,891 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-09 15:52:42,043 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=20
2025-06-09 15:52:42,043 - cogview4_api_server - INFO - [create_image:1269] - Starting streaming response
2025-06-09 15:52:42,047 - cogview4_api_server - INFO - [generate_sse_stream:1195] - Starting SSE stream generation
2025-06-09 15:52:42,595 - cogview4_api_server - INFO - [<module>:52] - Starting CogView4 API with log level: INFO
2025-06-09 15:52:42,595 - cogview4_api_server - INFO - [<module>:53] - Log file: cogview4_api.log
2025-06-09 15:52:42,596 - cogview4_api_server - INFO - [<module>:54] - Number of worker processes: 2
2025-06-09 15:52:42,606 - Worker1 - INFO - Worker 1: Processing batched request d7c08a18 with 1 prompts, stream=True
2025-06-09 15:53:01,897 - Worker1 - WARNING - Worker 1: Error decoding step 1: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 46.06 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:53:10,218 - Worker1 - WARNING - Worker 1: Error decoding step 2: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 46.06 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:53:18,611 - Worker1 - WARNING - Worker 1: Error decoding step 3: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 46.06 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:53:27,032 - Worker1 - WARNING - Worker 1: Error decoding step 4: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 46.06 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:53:35,632 - Worker1 - WARNING - Worker 1: Error decoding step 5: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 46.06 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:53:44,285 - Worker1 - WARNING - Worker 1: Error decoding step 6: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 46.06 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:53:52,965 - Worker1 - WARNING - Worker 1: Error decoding step 7: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.44 GiB is free. Including non-PyTorch memory, this process has 46.06 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:54:01,645 - Worker1 - WARNING - Worker 1: Error decoding step 8: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:54:10,363 - Worker1 - WARNING - Worker 1: Error decoding step 9: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:54:19,086 - Worker1 - WARNING - Worker 1: Error decoding step 10: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:54:27,819 - Worker1 - WARNING - Worker 1: Error decoding step 11: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:54:36,584 - Worker1 - WARNING - Worker 1: Error decoding step 12: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:54:45,407 - Worker1 - WARNING - Worker 1: Error decoding step 13: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:54:54,149 - Worker1 - WARNING - Worker 1: Error decoding step 14: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:02,904 - Worker1 - WARNING - Worker 1: Error decoding step 15: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:11,682 - Worker1 - WARNING - Worker 1: Error decoding step 16: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:20,520 - Worker1 - WARNING - Worker 1: Error decoding step 17: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:29,372 - Worker1 - WARNING - Worker 1: Error decoding step 18: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.50 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:38,237 - Worker1 - WARNING - Worker 1: Error decoding step 19: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:39,861 - Worker1 - ERROR - Worker 1: Error in batched streaming request: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/cogview4_api_server.py", line 539, in process_batched_streaming_request
    result = pipeline(
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 672, in __call__
    image = self.vae.decode(latents, return_dict=False, generator=generator)[0]
  File "/root/myprojects/diffusers/src/diffusers/utils/accelerate_utils.py", line 46, in wrapper
    return method(self, *args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py", line 323, in decode
    decoded = self._decode(z).sample
  File "/root/myprojects/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py", line 294, in _decode
    dec = self.decoder(z)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/autoencoders/vae.py", line 305, in forward
    sample = up_block(sample, latent_embeds)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/unets/unet_2d_blocks.py", line 2643, in forward
    hidden_states = upsampler(hidden_states)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/upsampling.py", line 188, in forward
    hidden_states = self.conv(hidden_states)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:39,885 - cogview4_api_server - ERROR - [_stream_results:1079] - Error from worker for request 40dd433f: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:55:39,886 - cogview4_api_server - ERROR - [generate_sse_stream:1245] - SSE stream error after 177.84s: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 46.04 GiB memory in use. Of the allocated memory 31.49 GiB is allocated by PyTorch, and 14.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:58:20,759 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=20
2025-06-09 15:58:20,760 - cogview4_api_server - INFO - [create_image:1269] - Starting streaming response
2025-06-09 15:58:20,761 - cogview4_api_server - INFO - [generate_sse_stream:1195] - Starting SSE stream generation
2025-06-09 15:58:21,359 - Worker0 - INFO - Worker 0: Processing batched request 64c46b42 with 1 prompts, stream=True
2025-06-09 16:00:08,684 - cogview4_api_server - INFO - [generate_sse_stream:1240] - SSE streaming completed in 107.92s
2025-06-09 16:00:31,755 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=20
2025-06-09 16:00:31,755 - cogview4_api_server - INFO - [create_image:1269] - Starting streaming response
2025-06-09 16:00:31,755 - cogview4_api_server - INFO - [generate_sse_stream:1195] - Starting SSE stream generation
2025-06-09 16:00:32,332 - Worker1 - INFO - Worker 1: Processing batched request 4c93661c with 1 prompts, stream=True
2025-06-09 16:02:03,963 - cogview4_api_server - INFO - [generate_sse_stream:1240] - SSE streaming completed in 92.21s
2025-06-09 16:02:47,708 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A steampunk workshop with gears and machines...', stream=False, steps=30
2025-06-09 16:02:47,811 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A bustling marketplace in medieval times...', stream=False, steps=30
2025-06-09 16:02:47,912 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='An underwater coral reef with colorful fish...', stream=False, steps=30
2025-06-09 16:02:48,013 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A space station orbiting Earth...', stream=False, steps=30
2025-06-09 16:02:48,114 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A cozy cabin in a snowy forest...', stream=False, steps=30
2025-06-09 16:02:48,215 - Worker1 - INFO - Worker 1: Processing batched request f7cb6421 with 5 prompts, stream=False
2025-06-09 16:02:48,215 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A desert oasis with palm trees...', stream=False, steps=30
2025-06-09 16:02:48,316 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A peaceful mountain landscape with a lake...', stream=False, steps=30
2025-06-09 16:02:48,417 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A futuristic city with flying cars...', stream=False, steps=30
2025-06-09 16:02:48,816 - Worker0 - INFO - Worker 0: Processing batched request 2d2f313a with 3 prompts, stream=False
2025-06-09 16:05:20,195 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=20
2025-06-09 16:05:20,196 - cogview4_api_server - INFO - [create_image:1269] - Starting streaming response
2025-06-09 16:05:20,899 - cogview4_api_server - INFO - [generate_sse_stream:1195] - Starting SSE stream generation
2025-06-09 16:05:30,493 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 162.28s: generated 1 images
2025-06-09 16:05:33,447 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 165.13s: generated 1 images
2025-06-09 16:05:35,322 - Worker0 - INFO - Worker 0: Processing batched request e34c1bfb with 1 prompts, stream=True
2025-06-09 16:05:35,331 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 166.91s: generated 1 images
2025-06-09 16:07:17,606 - Worker1 - ERROR - Worker 1: Error in batched non-streaming request: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,607 - cogview4_api_server - ERROR - [_wait_for_completion:1106] - Error from worker for request b06cd8de: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,608 - cogview4_api_server - ERROR - [create_image:1316] - Request failed after 269.90s: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,610 - cogview4_api_server - ERROR - [_wait_for_completion:1106] - Error from worker for request 78eabdcc: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,611 - cogview4_api_server - ERROR - [create_image:1316] - Request failed after 269.70s: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,611 - cogview4_api_server - ERROR - [_wait_for_completion:1106] - Error from worker for request 799b1cc3: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,611 - cogview4_api_server - ERROR - [create_image:1316] - Request failed after 269.60s: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,612 - cogview4_api_server - ERROR - [_wait_for_completion:1106] - Error from worker for request a4d7316a: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,612 - cogview4_api_server - ERROR - [create_image:1316] - Request failed after 269.50s: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,639 - cogview4_api_server - ERROR - [_wait_for_completion:1106] - Error from worker for request c51658db: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,639 - cogview4_api_server - ERROR - [create_image:1316] - Request failed after 269.83s: CUDA out of memory. Tried to allocate 10.00 GiB. GPU 1 has a total capacity of 47.51 GiB of which 9.46 GiB is free. Including non-PyTorch memory, this process has 38.04 GiB memory in use. Of the allocated memory 37.50 GiB is allocated by PyTorch, and 50.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:07:17,745 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A wizard casting spells in a library...', stream=True, steps=20
2025-06-09 16:07:17,745 - cogview4_api_server - INFO - [create_image:1269] - Starting streaming response
2025-06-09 16:07:17,745 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A dragon flying over mountains...', stream=True, steps=20
2025-06-09 16:07:17,745 - cogview4_api_server - INFO - [create_image:1269] - Starting streaming response
2025-06-09 16:07:17,745 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A robot walking through a neon city...', stream=True, steps=20
2025-06-09 16:07:17,745 - cogview4_api_server - INFO - [create_image:1269] - Starting streaming response
2025-06-09 16:07:17,746 - cogview4_api_server - INFO - [generate_sse_stream:1195] - Starting SSE stream generation
2025-06-09 16:07:17,847 - cogview4_api_server - INFO - [generate_sse_stream:1195] - Starting SSE stream generation
2025-06-09 16:07:17,948 - cogview4_api_server - INFO - [generate_sse_stream:1195] - Starting SSE stream generation
2025-06-09 16:07:18,294 - Worker1 - INFO - Worker 1: Processing batched request 18d08d53 with 3 prompts, stream=True
2025-06-09 16:07:26,207 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A mountain peak...', stream=False, steps=20
2025-06-09 16:07:26,208 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A forest in autumn...', stream=False, steps=20
2025-06-09 16:07:26,209 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A city at night...', stream=False, steps=20
2025-06-09 16:07:26,210 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A sunset over the ocean...', stream=False, steps=20
2025-06-09 16:07:48,189 - Worker0 - INFO - Worker 0: Processing batched request e39acff0 with 2 prompts, stream=False
2025-06-09 16:07:56,324 - cogview4_api_server - INFO - [generate_sse_stream:1240] - SSE streaming completed in 155.43s
2025-06-09 16:09:06,876 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 100.67s: generated 1 images
2025-06-09 16:09:08,089 - Worker0 - INFO - Worker 0: Processing batched request 1e60d8ca with 1 prompts, stream=False
2025-06-09 16:09:10,881 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 104.67s: generated 1 images
2025-06-09 16:09:48,089 - Worker0 - INFO - Worker 0: Processing batched request 7a283334 with 1 prompts, stream=False
2025-06-09 16:09:53,303 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 147.09s: generated 1 images
2025-06-09 16:10:04,498 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 158.29s: generated 1 images
2025-06-09 16:10:04,511 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A cute cat playing with yarn...', stream=False, steps=25
2025-06-09 16:10:04,512 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A modern kitchen with marble counters...', stream=False, steps=25
2025-06-09 16:10:04,513 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=False, steps=25
2025-06-09 16:10:05,014 - Worker0 - INFO - Worker 0: Processing batched request 3f6d74a9 with 3 prompts, stream=False
2025-06-09 16:12:22,785 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 138.27s: generated 1 images
2025-06-09 16:12:29,217 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 144.70s: generated 1 images
2025-06-09 16:12:32,035 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 147.52s: generated 1 images
2025-06-09 16:12:32,052 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A ocean view...', stream=False, steps=25
2025-06-09 16:12:32,053 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A forest scene...', stream=False, steps=25
2025-06-09 16:12:32,054 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A mountain landscape...', stream=False, steps=25
2025-06-09 16:12:32,618 - Worker0 - INFO - Worker 0: Processing batched request 7f14f5da with 3 prompts, stream=False
2025-06-09 16:14:49,867 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 137.81s: generated 1 images
2025-06-09 16:14:50,414 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 138.36s: generated 1 images
2025-06-09 16:14:53,698 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 141.64s: generated 1 images
2025-06-09 16:14:53,739 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A desert landscape...', stream=False, steps=25
2025-06-09 16:14:53,740 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A city skyline...', stream=False, steps=25
2025-06-09 16:14:54,329 - Worker1 - INFO - Worker 1: Processing batched request 77d8fa3c with 2 prompts, stream=False
2025-06-09 16:18:03,379 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 189.64s: generated 2 images
2025-06-09 16:18:07,078 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 193.34s: generated 2 images
2025-06-09 16:18:07,126 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A robot design...', stream=False, steps=25
2025-06-09 16:18:07,128 - cogview4_api_server - INFO - [create_image:1265] - Received image generation request: prompt='A space station...', stream=False, steps=25
2025-06-09 16:18:07,716 - Worker0 - INFO - Worker 0: Processing batched request 4c0e0000 with 1 prompts, stream=False
2025-06-09 16:18:07,717 - Worker1 - INFO - Worker 1: Processing batched request e01a7305 with 1 prompts, stream=False
2025-06-09 16:20:32,321 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 145.19s: generated 3 images
2025-06-09 16:20:34,151 - cogview4_api_server - INFO - [create_image:1307] - Non-streaming request completed in 147.02s: generated 1 images
2025-06-09 16:23:03,097 - cogview4_api_server - INFO - [lifespan:1165] - Shutting down CogView4 API server...
2025-06-09 16:23:03,097 - cogview4_api_server - INFO - [shutdown:1118] - Shutting down worker pool...
2025-06-09 16:23:04,074 - cogview4_api_server - INFO - [shutdown:1138] - Worker pool shutdown complete
2025-06-09 16:23:04,075 - cogview4_api_server - INFO - [lifespan:1168] - Worker pool shut down successfully
2025-06-09 16:23:10,547 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:23:10,547 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:23:10,547 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:23:10,547 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:23:10,571 - __main__ - INFO - [<module>:1465] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-09 16:23:10,597 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:23:10,597 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:23:10,597 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:23:10,597 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:23:10,702 - cogview4_api_server - INFO - [lifespan:1178] - Starting CogView4 API server...
2025-06-09 16:23:10,703 - cogview4_api_server - INFO - [__init__:894] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-09 16:23:14,507 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:23:14,507 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:23:14,507 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:23:14,507 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:23:14,526 - cogview4_api_server - INFO - [__init__:918] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-09 16:23:14,527 - cogview4_api_server - INFO - [__init__:931] - Started worker process 0 using spawn method
2025-06-09 16:23:14,528 - cogview4_api_server - INFO - [__init__:931] - Started worker process 1 using spawn method
2025-06-09 16:23:14,528 - cogview4_api_server - INFO - [__init__:933] - Worker pool initialized with 2 workers
2025-06-09 16:23:14,528 - cogview4_api_server - INFO - [__init__:943] - Prompt batching enabled - timeout checker started
2025-06-09 16:23:14,528 - cogview4_api_server - INFO - [lifespan:1181] - Worker pool initialized successfully!
2025-06-09 16:23:18,350 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:23:18,350 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:23:18,350 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:23:18,350 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:23:18,359 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:23:18,360 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:23:18,360 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:23:18,360 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:23:18,371 - worker_0 - INFO - [worker_process:178] - Worker 0 starting up...
2025-06-09 16:23:18,371 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-09 16:23:18,371 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-09 16:23:18,402 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:23:18,403 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:23:18,403 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:23:18,403 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:23:18,412 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:23:18,412 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:23:18,413 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:23:18,413 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:23:18,424 - worker_1 - INFO - [worker_process:178] - Worker 1 starting up...
2025-06-09 16:23:18,424 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-09 16:23:18,491 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-09 16:23:18,491 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-09 16:23:21,426 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-09 16:23:21,594 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-09 16:23:21,594 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-09 16:23:53,989 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-09 16:23:53,990 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-09 16:23:53,990 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-09 16:23:57,037 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-09 16:23:57,039 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-09 16:23:57,039 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-09 16:40:32,268 - cogview4_api_server - INFO - [lifespan:1188] - Shutting down CogView4 API server...
2025-06-09 16:40:32,268 - cogview4_api_server - INFO - [shutdown:1141] - Shutting down worker pool...
2025-06-09 16:40:33,069 - cogview4_api_server - INFO - [shutdown:1161] - Worker pool shutdown complete
2025-06-09 16:40:33,069 - cogview4_api_server - INFO - [lifespan:1191] - Worker pool shut down successfully
2025-06-09 16:40:39,976 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:40:39,976 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:40:39,976 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:40:39,976 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:40:40,000 - __main__ - INFO - [<module>:1538] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-09 16:40:40,026 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:40:40,027 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:40:40,027 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:40:40,027 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:40:40,133 - cogview4_api_server - INFO - [lifespan:1251] - Starting CogView4 API server...
2025-06-09 16:40:40,133 - cogview4_api_server - INFO - [__init__:927] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-09 16:40:43,940 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:40:43,940 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:40:43,940 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:40:43,940 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:40:43,958 - cogview4_api_server - INFO - [__init__:951] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-09 16:40:43,959 - cogview4_api_server - INFO - [__init__:964] - Started worker process 0 using spawn method
2025-06-09 16:40:43,960 - cogview4_api_server - INFO - [__init__:964] - Started worker process 1 using spawn method
2025-06-09 16:40:43,960 - cogview4_api_server - INFO - [__init__:966] - Worker pool initialized with 2 workers
2025-06-09 16:40:43,961 - cogview4_api_server - INFO - [__init__:976] - Prompt batching enabled - timeout checker started
2025-06-09 16:40:43,961 - cogview4_api_server - INFO - [_monitor_worker_readiness:1208] - Starting worker readiness monitoring...
2025-06-09 16:40:43,961 - cogview4_api_server - INFO - [__init__:984] - Worker readiness monitor started
2025-06-09 16:40:43,962 - cogview4_api_server - INFO - [lifespan:1254] - Worker pool initialized successfully!
2025-06-09 16:40:47,772 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:40:47,772 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:40:47,772 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:40:47,772 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:40:47,782 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:40:47,782 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:40:47,782 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:40:47,782 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:40:47,793 - worker_1 - INFO - [worker_process:211] - Worker 1 starting up...
2025-06-09 16:40:47,793 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-09 16:40:47,891 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:40:47,891 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:40:47,891 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:40:47,891 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:40:47,901 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:40:47,901 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:40:47,901 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:40:47,901 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:40:47,912 - worker_0 - INFO - [worker_process:211] - Worker 0 starting up...
2025-06-09 16:40:47,913 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-09 16:40:47,913 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-09 16:40:48,027 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-09 16:40:48,027 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-09 16:40:50,796 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-09 16:40:50,946 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-09 16:40:50,946 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-09 16:41:18,988 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-09 16:41:18,990 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-09 16:41:18,990 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-09 16:41:20,007 - cogview4_api_server - INFO - [_monitor_worker_readiness:1224] - Worker loading progress: 1/2 workers ready
2025-06-09 16:41:26,721 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-09 16:41:26,723 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-09 16:41:26,723 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-09 16:41:28,015 - cogview4_api_server - INFO - [_monitor_worker_readiness:1217] - All workers have loaded models - displaying ready banner
2025-06-09 16:41:28,016 - cogview4_api_server - INFO - [display_ready_banner:89] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-09 16:41:28,016 - cogview4_api_server - INFO - [display_ready_banner:90] -  2 workers ready to process requests
2025-06-09 16:41:45,227 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=50
2025-06-09 16:41:45,227 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-09 16:41:45,230 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-09 16:41:45,763 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:41:45,763 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:41:45,763 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:41:45,763 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:41:45,772 - Worker0 - INFO - Worker 0: Processing batched request 6a4cba37 with 1 prompts, stream=True
2025-06-09 16:42:02,360 - Worker0 - WARNING - Worker 0: Error decoding step 1: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:09,388 - Worker0 - WARNING - Worker 0: Error decoding step 2: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:16,437 - Worker0 - WARNING - Worker 0: Error decoding step 3: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:23,499 - Worker0 - WARNING - Worker 0: Error decoding step 4: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:30,568 - Worker0 - WARNING - Worker 0: Error decoding step 5: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:37,664 - Worker0 - WARNING - Worker 0: Error decoding step 6: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:44,780 - Worker0 - WARNING - Worker 0: Error decoding step 7: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:51,908 - Worker0 - WARNING - Worker 0: Error decoding step 8: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:42:59,045 - Worker0 - WARNING - Worker 0: Error decoding step 9: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:06,195 - Worker0 - WARNING - Worker 0: Error decoding step 10: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:13,343 - Worker0 - WARNING - Worker 0: Error decoding step 11: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:20,497 - Worker0 - WARNING - Worker 0: Error decoding step 12: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:27,830 - Worker0 - WARNING - Worker 0: Error decoding step 13: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:35,195 - Worker0 - WARNING - Worker 0: Error decoding step 14: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:42,564 - Worker0 - WARNING - Worker 0: Error decoding step 15: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:49,930 - Worker0 - WARNING - Worker 0: Error decoding step 16: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:43:57,307 - Worker0 - WARNING - Worker 0: Error decoding step 17: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:04,689 - Worker0 - WARNING - Worker 0: Error decoding step 18: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:12,086 - Worker0 - WARNING - Worker 0: Error decoding step 19: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:19,500 - Worker0 - WARNING - Worker 0: Error decoding step 20: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:26,920 - Worker0 - WARNING - Worker 0: Error decoding step 21: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:34,355 - Worker0 - WARNING - Worker 0: Error decoding step 22: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:41,773 - Worker0 - WARNING - Worker 0: Error decoding step 23: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:49,219 - Worker0 - WARNING - Worker 0: Error decoding step 24: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:44:56,669 - Worker0 - WARNING - Worker 0: Error decoding step 25: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:04,128 - Worker0 - WARNING - Worker 0: Error decoding step 26: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:11,582 - Worker0 - WARNING - Worker 0: Error decoding step 27: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:19,001 - Worker0 - WARNING - Worker 0: Error decoding step 28: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:26,422 - Worker0 - WARNING - Worker 0: Error decoding step 29: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:33,823 - Worker0 - WARNING - Worker 0: Error decoding step 30: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:41,229 - Worker0 - WARNING - Worker 0: Error decoding step 31: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:48,649 - Worker0 - WARNING - Worker 0: Error decoding step 32: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:45:56,075 - Worker0 - WARNING - Worker 0: Error decoding step 33: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:03,496 - Worker0 - WARNING - Worker 0: Error decoding step 34: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:10,931 - Worker0 - WARNING - Worker 0: Error decoding step 35: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:18,368 - Worker0 - WARNING - Worker 0: Error decoding step 36: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:25,806 - Worker0 - WARNING - Worker 0: Error decoding step 37: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:33,239 - Worker0 - WARNING - Worker 0: Error decoding step 38: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:40,681 - Worker0 - WARNING - Worker 0: Error decoding step 39: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:48,123 - Worker0 - WARNING - Worker 0: Error decoding step 40: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:46:55,571 - Worker0 - WARNING - Worker 0: Error decoding step 41: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:03,014 - Worker0 - WARNING - Worker 0: Error decoding step 42: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:10,462 - Worker0 - WARNING - Worker 0: Error decoding step 43: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:17,913 - Worker0 - WARNING - Worker 0: Error decoding step 44: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:25,359 - Worker0 - WARNING - Worker 0: Error decoding step 45: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:32,804 - Worker0 - WARNING - Worker 0: Error decoding step 46: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:40,251 - Worker0 - WARNING - Worker 0: Error decoding step 47: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:47,715 - Worker0 - WARNING - Worker 0: Error decoding step 48: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:55,169 - Worker0 - WARNING - Worker 0: Error decoding step 49: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:56,581 - Worker0 - ERROR - Worker 0: Error in batched streaming request: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/cogview4_api_server.py", line 576, in process_batched_streaming_request
    for neg_prompt in batch_request.negative_prompts
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 672, in __call__
    image = self.vae.decode(latents, return_dict=False, generator=generator)[0]
  File "/root/myprojects/diffusers/src/diffusers/utils/accelerate_utils.py", line 46, in wrapper
    return method(self, *args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py", line 323, in decode
    decoded = self._decode(z).sample
  File "/root/myprojects/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py", line 294, in _decode
    dec = self.decoder(z)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/autoencoders/vae.py", line 305, in forward
    sample = up_block(sample, latent_embeds)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/unets/unet_2d_blocks.py", line 2643, in forward
    hidden_states = upsampler(hidden_states)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/src/diffusers/models/upsampling.py", line 188, in forward
    hidden_states = self.conv(hidden_states)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/root/myprojects/diffusers/examples/server/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:56,585 - cogview4_api_server - ERROR - [_stream_results:1145] - Error from worker for request a6812cbe: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:47:56,586 - cogview4_api_server - ERROR - [generate_sse_stream:1341] - SSE stream error after 371.36s: CUDA out of memory. Tried to allocate 7.03 GiB. GPU 0 has a total capacity of 47.51 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 42.67 GiB memory in use. Of the allocated memory 29.80 GiB is allocated by PyTorch, and 12.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 16:48:31,649 - cogview4_api_server - INFO - [lifespan:1261] - Shutting down CogView4 API server...
2025-06-09 16:48:31,650 - cogview4_api_server - INFO - [shutdown:1184] - Shutting down worker pool...
2025-06-09 16:48:32,437 - cogview4_api_server - INFO - [shutdown:1204] - Worker pool shutdown complete
2025-06-09 16:48:32,438 - cogview4_api_server - INFO - [lifespan:1264] - Worker pool shut down successfully
2025-06-09 16:48:39,263 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:48:39,263 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:48:39,263 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:48:39,263 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:48:39,287 - __main__ - INFO - [<module>:1542] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-09 16:48:39,314 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:48:39,314 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:48:39,314 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:48:39,314 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:48:39,420 - cogview4_api_server - INFO - [lifespan:1255] - Starting CogView4 API server...
2025-06-09 16:48:39,420 - cogview4_api_server - INFO - [__init__:931] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-09 16:48:43,220 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:48:43,220 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:48:43,220 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:48:43,220 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:48:43,239 - cogview4_api_server - INFO - [__init__:955] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-09 16:48:43,240 - cogview4_api_server - INFO - [__init__:968] - Started worker process 0 using spawn method
2025-06-09 16:48:43,241 - cogview4_api_server - INFO - [__init__:968] - Started worker process 1 using spawn method
2025-06-09 16:48:43,241 - cogview4_api_server - INFO - [__init__:970] - Worker pool initialized with 2 workers
2025-06-09 16:48:43,241 - cogview4_api_server - INFO - [__init__:980] - Prompt batching enabled - timeout checker started
2025-06-09 16:48:43,242 - cogview4_api_server - INFO - [_monitor_worker_readiness:1212] - Starting worker readiness monitoring...
2025-06-09 16:48:43,242 - cogview4_api_server - INFO - [__init__:988] - Worker readiness monitor started
2025-06-09 16:48:43,242 - cogview4_api_server - INFO - [lifespan:1258] - Worker pool initialized successfully!
2025-06-09 16:48:47,051 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:48:47,051 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:48:47,051 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:48:47,051 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:48:47,060 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:48:47,061 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:48:47,061 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:48:47,061 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:48:47,071 - worker_1 - INFO - [worker_process:211] - Worker 1 starting up...
2025-06-09 16:48:47,072 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-09 16:48:47,135 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:48:47,135 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:48:47,136 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:48:47,136 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:48:47,145 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:48:47,145 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:48:47,145 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:48:47,145 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:48:47,156 - worker_0 - INFO - [worker_process:211] - Worker 0 starting up...
2025-06-09 16:48:47,157 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-09 16:48:47,157 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-09 16:48:47,269 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-09 16:48:47,269 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-09 16:48:50,075 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-09 16:48:50,225 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-09 16:48:50,225 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-09 16:49:32,033 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-09 16:49:32,035 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-09 16:49:32,035 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-09 16:49:33,298 - cogview4_api_server - INFO - [_monitor_worker_readiness:1228] - Worker loading progress: 1/2 workers ready
2025-06-09 16:49:36,244 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-09 16:49:36,245 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-09 16:49:36,245 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-09 16:49:37,303 - cogview4_api_server - INFO - [_monitor_worker_readiness:1221] - All workers have loaded models - displaying ready banner
2025-06-09 16:49:37,304 - cogview4_api_server - INFO - [display_ready_banner:89] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-09 16:49:37,304 - cogview4_api_server - INFO - [display_ready_banner:90] -  2 workers ready to process requests
2025-06-09 16:50:07,559 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=15
2025-06-09 16:50:07,559 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 16:50:07,563 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 16:50:08,073 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:50:08,073 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:50:08,073 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:50:08,073 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:50:08,082 - Worker1 - INFO - Worker 1: Processing batched request 522504a1 with 1 prompts, stream=True
2025-06-09 16:53:43,121 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 215.56s
2025-06-09 16:53:50,486 - cogview4_api_server - INFO - [lifespan:1265] - Shutting down CogView4 API server...
2025-06-09 16:53:50,486 - cogview4_api_server - INFO - [shutdown:1188] - Shutting down worker pool...
2025-06-09 16:53:52,268 - cogview4_api_server - INFO - [shutdown:1208] - Worker pool shutdown complete
2025-06-09 16:53:52,268 - cogview4_api_server - INFO - [lifespan:1268] - Worker pool shut down successfully
2025-06-09 16:53:58,838 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:53:58,838 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:53:58,838 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:53:58,838 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:53:58,862 - __main__ - INFO - [<module>:1542] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-09 16:53:58,889 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:53:58,889 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:53:58,889 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:53:58,889 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:53:58,995 - cogview4_api_server - INFO - [lifespan:1255] - Starting CogView4 API server...
2025-06-09 16:53:58,995 - cogview4_api_server - INFO - [__init__:931] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-09 16:54:02,829 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:54:02,829 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:54:02,829 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:54:02,829 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:54:02,848 - cogview4_api_server - INFO - [__init__:955] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-09 16:54:02,849 - cogview4_api_server - INFO - [__init__:968] - Started worker process 0 using spawn method
2025-06-09 16:54:02,850 - cogview4_api_server - INFO - [__init__:968] - Started worker process 1 using spawn method
2025-06-09 16:54:02,850 - cogview4_api_server - INFO - [__init__:970] - Worker pool initialized with 2 workers
2025-06-09 16:54:02,850 - cogview4_api_server - INFO - [__init__:980] - Prompt batching enabled - timeout checker started
2025-06-09 16:54:02,851 - cogview4_api_server - INFO - [_monitor_worker_readiness:1212] - Starting worker readiness monitoring...
2025-06-09 16:54:02,851 - cogview4_api_server - INFO - [__init__:988] - Worker readiness monitor started
2025-06-09 16:54:02,851 - cogview4_api_server - INFO - [lifespan:1258] - Worker pool initialized successfully!
2025-06-09 16:54:06,650 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:54:06,650 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:54:06,650 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:54:06,650 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:54:06,660 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:54:06,660 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:54:06,660 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:54:06,660 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:54:06,671 - worker_0 - INFO - [worker_process:211] - Worker 0 starting up...
2025-06-09 16:54:06,671 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-09 16:54:06,671 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-09 16:54:06,762 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:54:06,762 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:54:06,762 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:54:06,762 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:54:06,772 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:54:06,772 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:54:06,772 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:54:06,772 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:54:06,783 - worker_1 - INFO - [worker_process:211] - Worker 1 starting up...
2025-06-09 16:54:06,783 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-09 16:54:06,791 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-09 16:54:06,791 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-09 16:54:09,785 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-09 16:54:09,939 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-09 16:54:09,940 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-09 16:54:50,433 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-09 16:54:50,435 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-09 16:54:50,435 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-09 16:54:50,916 - cogview4_api_server - INFO - [_monitor_worker_readiness:1228] - Worker loading progress: 1/2 workers ready
2025-06-09 16:54:57,436 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-09 16:54:57,437 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-09 16:54:57,437 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-09 16:54:58,926 - cogview4_api_server - INFO - [_monitor_worker_readiness:1221] - All workers have loaded models - displaying ready banner
2025-06-09 16:54:58,926 - cogview4_api_server - INFO - [display_ready_banner:89] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-09 16:54:58,926 - cogview4_api_server - INFO - [display_ready_banner:90] -  2 workers ready to process requests
2025-06-09 16:56:07,545 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=15
2025-06-09 16:56:07,545 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 16:56:07,550 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 16:56:08,151 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 16:56:08,152 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 16:56:08,152 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 16:56:08,152 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 16:56:08,160 - Worker0 - INFO - Worker 0: Processing batched request 36dba4fe with 1 prompts, stream=True
2025-06-09 16:58:33,087 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A bustling marketplace in medieval times...', stream=False, steps=30
2025-06-09 16:58:33,188 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='An underwater coral reef with colorful fish...', stream=False, steps=30
2025-06-09 16:58:33,289 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A steampunk workshop with gears and machines...', stream=False, steps=30
2025-06-09 16:58:33,390 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A space station orbiting Earth...', stream=False, steps=30
2025-06-09 16:58:33,391 - cogview4_api_server - INFO - [submit_non_streaming_request:1116] - Submitted batch fb93cf4b with 3 prompts (including 6e8c21ea)
2025-06-09 16:58:33,391 - Worker1 - INFO - Worker 1: Processing batched request fb93cf4b with 3 prompts, stream=False
2025-06-09 16:58:33,492 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A cozy cabin in a snowy forest...', stream=False, steps=30
2025-06-09 16:58:33,594 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A desert oasis with palm trees...', stream=False, steps=30
2025-06-09 16:58:33,695 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A peaceful mountain landscape with a lake...', stream=False, steps=30
2025-06-09 16:58:33,696 - cogview4_api_server - INFO - [submit_non_streaming_request:1116] - Submitted batch 17e5d194 with 3 prompts (including 427fbd9e)
2025-06-09 16:58:33,798 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A futuristic city with flying cars...', stream=False, steps=30
2025-06-09 17:00:46,103 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 278.55s
2025-06-09 17:00:46,122 - Worker0 - INFO - Worker 0: Processing batched request 17e5d194 with 3 prompts, stream=False
2025-06-09 17:03:35,592 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A wizard casting spells in a library...', stream=True, steps=20
2025-06-09 17:03:35,593 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:03:35,594 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A dragon flying over mountains...', stream=True, steps=20
2025-06-09 17:03:35,594 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:03:35,594 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A robot walking through a neon city...', stream=True, steps=20
2025-06-09 17:03:35,594 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:03:35,594 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:03:35,695 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:03:35,796 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:04:05,409 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 332.32s: generated 1 images
2025-06-09 17:04:07,629 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 334.44s: generated 1 images
2025-06-09 17:04:08,966 - Worker1 - INFO - Worker 1: Processing batched request 4f6f5708 with 2 prompts, stream=False
2025-06-09 17:04:08,974 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 335.68s: generated 1 images
2025-06-09 17:06:21,887 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 468.50s: generated 1 images
2025-06-09 17:06:22,736 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 469.24s: generated 1 images
2025-06-09 17:06:25,323 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 471.73s: generated 1 images
2025-06-09 17:06:25,326 - Worker0 - INFO - Worker 0: Processing batched request b65318f0 with 3 prompts, stream=True
2025-06-09 17:06:56,483 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A mountain peak...', stream=False, steps=20
2025-06-09 17:06:56,484 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A forest in autumn...', stream=False, steps=20
2025-06-09 17:06:56,485 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A city at night...', stream=False, steps=20
2025-06-09 17:06:56,486 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A sunset over the ocean...', stream=False, steps=20
2025-06-09 17:08:18,465 - Worker0 - WARNING - Worker 0: Error decoding step 5: CUDA out of memory. Tried to allocate 6.00 GiB. GPU 0 has a total capacity of 47.51 GiB of which 3.64 GiB is free. Including non-PyTorch memory, this process has 13.48 GiB memory in use. Process 1879020 has 30.38 GiB memory in use. Of the allocated memory 12.04 GiB is allocated by PyTorch, and 968.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 17:08:18,763 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 585.07s: generated 1 images
2025-06-09 17:08:20,401 - Worker1 - INFO - Worker 1: Processing batched request ddf4167a with 2 prompts, stream=False
2025-06-09 17:08:21,162 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 587.36s: generated 1 images
2025-06-09 17:11:07,001 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 250.52s: generated 1 images
2025-06-09 17:11:09,402 - Worker1 - INFO - Worker 1: Processing batched request 4765ab65 with 1 prompts, stream=False
2025-06-09 17:11:11,323 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 254.84s: generated 1 images
2025-06-09 17:12:09,605 - Worker0 - INFO - Worker 0: Processing batched request ef6d391e with 1 prompts, stream=False
2025-06-09 17:12:33,850 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 337.37s: generated 1 images
2025-06-09 17:12:47,712 - cogview4_api_server - INFO - [create_image:1420] - Non-streaming request completed in 351.23s: generated 1 images
2025-06-09 17:14:05,869 - cogview4_api_server - INFO - [lifespan:1265] - Shutting down CogView4 API server...
2025-06-09 17:14:05,870 - cogview4_api_server - INFO - [shutdown:1188] - Shutting down worker pool...
2025-06-09 17:14:07,750 - cogview4_api_server - INFO - [shutdown:1208] - Worker pool shutdown complete
2025-06-09 17:14:07,750 - cogview4_api_server - INFO - [lifespan:1268] - Worker pool shut down successfully
2025-06-09 17:14:18,225 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:14:18,225 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:14:18,225 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:14:18,225 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:14:18,249 - __main__ - INFO - [<module>:1542] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-09 17:14:18,276 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:14:18,276 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:14:18,276 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:14:18,276 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:14:18,383 - cogview4_api_server - INFO - [lifespan:1255] - Starting CogView4 API server...
2025-06-09 17:14:18,383 - cogview4_api_server - INFO - [__init__:931] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-09 17:14:22,192 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:14:22,192 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:14:22,192 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:14:22,192 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:14:22,211 - cogview4_api_server - INFO - [__init__:955] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-09 17:14:22,212 - cogview4_api_server - INFO - [__init__:968] - Started worker process 0 using spawn method
2025-06-09 17:14:22,213 - cogview4_api_server - INFO - [__init__:968] - Started worker process 1 using spawn method
2025-06-09 17:14:22,213 - cogview4_api_server - INFO - [__init__:970] - Worker pool initialized with 2 workers
2025-06-09 17:14:22,214 - cogview4_api_server - INFO - [__init__:980] - Prompt batching enabled - timeout checker started
2025-06-09 17:14:22,214 - cogview4_api_server - INFO - [_monitor_worker_readiness:1212] - Starting worker readiness monitoring...
2025-06-09 17:14:22,214 - cogview4_api_server - INFO - [__init__:988] - Worker readiness monitor started
2025-06-09 17:14:22,214 - cogview4_api_server - INFO - [lifespan:1258] - Worker pool initialized successfully!
2025-06-09 17:14:26,042 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:14:26,042 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:14:26,042 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:14:26,042 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:14:26,051 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:14:26,051 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:14:26,051 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:14:26,051 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:14:26,062 - worker_0 - INFO - [worker_process:211] - Worker 0 starting up...
2025-06-09 17:14:26,063 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-09 17:14:26,063 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-09 17:14:26,095 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:14:26,095 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:14:26,095 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:14:26,095 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:14:26,105 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:14:26,105 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:14:26,105 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:14:26,105 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:14:26,116 - worker_1 - INFO - [worker_process:211] - Worker 1 starting up...
2025-06-09 17:14:26,116 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-09 17:14:26,184 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-09 17:14:26,184 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-09 17:14:29,117 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-09 17:14:29,263 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-09 17:14:29,263 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-09 17:14:59,228 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-09 17:14:59,229 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-09 17:14:59,229 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-09 17:15:00,261 - cogview4_api_server - INFO - [_monitor_worker_readiness:1228] - Worker loading progress: 1/2 workers ready
2025-06-09 17:15:04,508 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-09 17:15:04,509 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-09 17:15:04,510 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-09 17:15:06,267 - cogview4_api_server - INFO - [_monitor_worker_readiness:1221] - All workers have loaded models - displaying ready banner
2025-06-09 17:15:06,268 - cogview4_api_server - INFO - [display_ready_banner:89] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-09 17:15:06,268 - cogview4_api_server - INFO - [display_ready_banner:90] -  2 workers ready to process requests
2025-06-09 17:15:29,854 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=15
2025-06-09 17:15:29,854 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:15:29,857 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:15:30,419 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:15:30,419 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:15:30,419 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:15:30,419 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:15:30,428 - Worker1 - INFO - Worker 1: Processing batched request b605c40a with 1 prompts, stream=True
2025-06-09 17:17:12,138 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 102.28s
2025-06-09 17:18:30,944 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=15
2025-06-09 17:18:30,944 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:18:30,945 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:18:31,463 - Worker1 - INFO - Worker 1: Processing batched request a55eb320 with 1 prompts, stream=True
2025-06-09 17:19:07,090 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 36.15s
2025-06-09 17:20:39,661 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=15
2025-06-09 17:20:39,662 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:20:39,663 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:20:40,222 - Worker0 - INFO - Worker 0: Processing batched request 0259a6f8 with 1 prompts, stream=True
2025-06-09 17:21:15,999 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 36.34s
2025-06-09 17:21:46,418 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=15
2025-06-09 17:21:46,418 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:21:46,419 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:21:47,006 - Worker0 - INFO - Worker 0: Processing batched request 4b598d90 with 1 prompts, stream=True
2025-06-09 17:23:05,800 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 79.38s
2025-06-09 17:23:24,932 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-09 17:23:24,932 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 17:23:24,933 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 17:23:25,528 - Worker0 - INFO - Worker 0: Processing batched request ad6e7b83 with 1 prompts, stream=True
2025-06-09 17:24:45,557 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 80.62s
2025-06-09 17:24:55,626 - cogview4_api_server - INFO - [lifespan:1265] - Shutting down CogView4 API server...
2025-06-09 17:24:55,626 - cogview4_api_server - INFO - [shutdown:1188] - Shutting down worker pool...
2025-06-09 17:24:56,489 - cogview4_api_server - INFO - [shutdown:1208] - Worker pool shutdown complete
2025-06-09 17:24:56,489 - cogview4_api_server - INFO - [lifespan:1268] - Worker pool shut down successfully
2025-06-09 17:58:00,384 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:58:00,384 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:58:00,384 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:58:00,384 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:58:00,408 - __main__ - INFO - [<module>:1542] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-09 17:58:00,427 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:58:00,427 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:58:00,427 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:58:00,427 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:58:00,534 - cogview4_api_server - INFO - [lifespan:1255] - Starting CogView4 API server...
2025-06-09 17:58:00,534 - cogview4_api_server - INFO - [__init__:931] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-09 17:58:04,336 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:58:04,336 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:58:04,336 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:58:04,336 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:58:04,355 - cogview4_api_server - INFO - [__init__:955] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-09 17:58:04,357 - cogview4_api_server - INFO - [__init__:968] - Started worker process 0 using spawn method
2025-06-09 17:58:04,358 - cogview4_api_server - INFO - [__init__:968] - Started worker process 1 using spawn method
2025-06-09 17:58:04,358 - cogview4_api_server - INFO - [__init__:970] - Worker pool initialized with 2 workers
2025-06-09 17:58:04,358 - cogview4_api_server - INFO - [__init__:980] - Prompt batching enabled - timeout checker started
2025-06-09 17:58:04,358 - cogview4_api_server - INFO - [_monitor_worker_readiness:1212] - Starting worker readiness monitoring...
2025-06-09 17:58:04,358 - cogview4_api_server - INFO - [__init__:988] - Worker readiness monitor started
2025-06-09 17:58:04,359 - cogview4_api_server - INFO - [lifespan:1258] - Worker pool initialized successfully!
2025-06-09 17:58:08,175 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:58:08,176 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:58:08,176 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:58:08,176 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:58:08,185 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:58:08,185 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:58:08,185 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:58:08,185 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:58:08,196 - worker_0 - INFO - [worker_process:211] - Worker 0 starting up...
2025-06-09 17:58:08,196 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-09 17:58:08,197 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-09 17:58:08,310 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:58:08,310 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:58:08,310 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:58:08,310 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:58:08,319 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-09 17:58:08,320 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-09 17:58:08,320 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 17:58:08,320 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 17:58:08,320 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 17:58:08,320 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 17:58:08,331 - worker_1 - INFO - [worker_process:211] - Worker 1 starting up...
2025-06-09 17:58:08,332 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-09 17:58:11,333 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-09 17:58:11,483 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-09 17:58:11,484 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-09 17:58:46,040 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-09 17:58:46,043 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-09 17:58:46,044 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-09 17:58:46,407 - cogview4_api_server - INFO - [_monitor_worker_readiness:1228] - Worker loading progress: 1/2 workers ready
2025-06-09 17:58:48,155 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-09 17:58:48,156 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-09 17:58:48,157 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-09 17:58:48,410 - cogview4_api_server - INFO - [_monitor_worker_readiness:1221] - All workers have loaded models - displaying ready banner
2025-06-09 17:58:48,410 - cogview4_api_server - INFO - [display_ready_banner:89] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-09 17:58:48,410 - cogview4_api_server - INFO - [display_ready_banner:90] -  2 workers ready to process requests
2025-06-09 18:00:01,840 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=25
2025-06-09 18:00:01,840 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:00:01,844 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:00:02,435 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-09 18:00:02,435 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-09 18:00:02,435 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-09 18:00:02,435 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-09 18:00:02,444 - Worker0 - INFO - Worker 0: Processing batched request 6e3495e9 with 1 prompts, stream=True
2025-06-09 18:01:10,605 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 68.76s
2025-06-09 18:01:35,241 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=25
2025-06-09 18:01:35,241 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:01:35,242 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:01:35,764 - Worker1 - INFO - Worker 1: Processing batched request ae9808b2 with 1 prompts, stream=True
2025-06-09 18:04:48,227 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 192.98s
2025-06-09 18:40:32,349 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:40:32,349 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:40:32,350 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:40:32,936 - Worker1 - INFO - Worker 1: Processing batched request 1930e2ba with 1 prompts, stream=True
2025-06-09 18:41:40,237 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 67.89s
2025-06-09 18:42:16,399 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:42:16,399 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:42:16,400 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:42:16,974 - Worker1 - INFO - Worker 1: Processing batched request ff0bf7b1 with 1 prompts, stream=True
2025-06-09 18:43:17,866 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 61.47s
2025-06-09 18:44:09,058 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:44:09,058 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:44:09,059 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:44:09,619 - Worker1 - INFO - Worker 1: Processing batched request 8d050f55 with 1 prompts, stream=True
2025-06-09 18:45:11,220 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 62.16s
2025-06-09 18:46:37,112 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:46:37,112 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:46:37,113 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:46:37,713 - Worker0 - INFO - Worker 0: Processing batched request 8a4ece41 with 1 prompts, stream=True
2025-06-09 18:46:57,471 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 20.36s
2025-06-09 18:47:56,836 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:47:56,836 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:47:56,837 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:47:57,418 - Worker0 - INFO - Worker 0: Processing batched request a9455bce with 1 prompts, stream=True
2025-06-09 18:48:17,276 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 20.44s
2025-06-09 18:48:31,588 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:48:31,588 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:48:31,589 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:48:32,165 - Worker1 - INFO - Worker 1: Processing batched request dc6eacdd with 1 prompts, stream=True
2025-06-09 18:48:51,765 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 20.18s
2025-06-09 18:49:25,610 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:49:25,610 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:49:25,611 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:49:26,137 - Worker1 - INFO - Worker 1: Processing batched request a908aee4 with 1 prompts, stream=True
2025-06-09 18:49:45,795 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 20.18s
2025-06-09 18:49:58,301 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:49:58,301 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:49:58,302 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:49:58,881 - Worker0 - INFO - Worker 0: Processing batched request 0092a171 with 1 prompts, stream=True
2025-06-09 18:50:18,620 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 20.32s
2025-06-09 18:50:45,081 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=25
2025-06-09 18:50:45,081 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:50:45,082 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:50:45,643 - Worker1 - INFO - Worker 1: Processing batched request 3601d93d with 1 prompts, stream=True
2025-06-09 18:51:05,110 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 20.03s
2025-06-09 18:52:10,373 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-09 18:52:10,373 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:52:10,374 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:52:10,956 - Worker1 - INFO - Worker 1: Processing batched request 3b9d568c with 1 prompts, stream=True
2025-06-09 18:52:34,061 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.69s
2025-06-09 18:53:57,114 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-09 18:53:57,114 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:53:57,115 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:53:57,699 - Worker1 - INFO - Worker 1: Processing batched request 3f2f061a with 1 prompts, stream=True
2025-06-09 18:54:21,070 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.95s
2025-06-09 18:54:40,034 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-09 18:54:40,035 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:54:40,035 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:54:40,557 - Worker0 - INFO - Worker 0: Processing batched request e34ef933 with 1 prompts, stream=True
2025-06-09 18:55:04,244 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 24.21s
2025-06-09 18:55:59,630 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-09 18:55:59,630 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 18:55:59,631 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 18:56:00,163 - Worker0 - INFO - Worker 0: Processing batched request c99aa08b with 1 prompts, stream=True
2025-06-09 18:56:23,849 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 24.22s
2025-06-09 19:00:19,426 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='
...', stream=True, steps=30
2025-06-09 19:00:19,427 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:00:19,428 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:00:20,021 - Worker1 - INFO - Worker 1: Processing batched request b7f9cfe6 with 1 prompts, stream=True
2025-06-09 19:00:42,701 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.27s
2025-06-09 19:12:12,244 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='
...', stream=True, steps=30
2025-06-09 19:12:12,245 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:12:12,246 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:12:12,782 - Worker1 - INFO - Worker 1: Processing batched request c446daa2 with 1 prompts, stream=True
2025-06-09 19:12:35,649 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.40s
2025-06-09 19:12:55,425 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='
...', stream=True, steps=30
2025-06-09 19:12:55,426 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:12:55,427 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:12:55,941 - Worker0 - INFO - Worker 0: Processing batched request ff334b48 with 1 prompts, stream=True
2025-06-09 19:13:19,140 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.71s
2025-06-09 19:13:35,237 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='
...', stream=True, steps=30
2025-06-09 19:13:35,238 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:13:35,238 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:13:35,794 - Worker0 - INFO - Worker 0: Processing batched request 64771601 with 1 prompts, stream=True
2025-06-09 19:13:58,675 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.44s
2025-06-09 19:14:38,829 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='
...', stream=True, steps=30
2025-06-09 19:14:38,829 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:14:38,829 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:14:39,381 - Worker1 - INFO - Worker 1: Processing batched request a6906aff with 1 prompts, stream=True
2025-06-09 19:15:02,077 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.25s
2025-06-09 19:16:29,490 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='
...', stream=True, steps=30
2025-06-09 19:16:29,491 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:16:29,491 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:16:30,030 - Worker1 - INFO - Worker 1: Processing batched request 5ceff6d5 with 1 prompts, stream=True
2025-06-09 19:16:53,134 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.64s
2025-06-09 19:18:16,712 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='
...', stream=True, steps=30
2025-06-09 19:18:16,713 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:18:16,713 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:18:17,276 - Worker0 - INFO - Worker 0: Processing batched request 3c12209b with 1 prompts, stream=True
2025-06-09 19:18:40,468 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 23.75s
2025-06-09 19:20:05,703 - cogview4_api_server - INFO - [create_image:1365] - Received image generation request: prompt='UNG...', stream=True, steps=50
2025-06-09 19:20:05,703 - cogview4_api_server - INFO - [create_image:1382] - Starting streaming response
2025-06-09 19:20:05,704 - cogview4_api_server - INFO - [generate_sse_stream:1295] - Starting SSE stream generation
2025-06-09 19:20:06,223 - Worker0 - INFO - Worker 0: Processing batched request d6e2dc38 with 1 prompts, stream=True
2025-06-09 19:22:19,233 - cogview4_api_server - INFO - [generate_sse_stream:1340] - SSE streaming completed in 133.53s
2025-06-10 10:46:56,546 - cogview4_api_server - INFO - [lifespan:1265] - Shutting down CogView4 API server...
2025-06-10 10:46:56,546 - cogview4_api_server - INFO - [shutdown:1188] - Shutting down worker pool...
2025-06-10 10:46:57,508 - cogview4_api_server - INFO - [shutdown:1208] - Worker pool shutdown complete
2025-06-10 10:46:57,509 - cogview4_api_server - INFO - [lifespan:1268] - Worker pool shut down successfully
2025-06-20 15:14:15,279 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:14:15,279 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:14:15,279 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:14:15,279 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:14:15,300 - __main__ - INFO - [<module>:1538] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-20 15:14:15,326 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:14:15,326 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:14:15,326 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:14:15,326 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:14:15,332 - cogview4_api_server - INFO - [lifespan:1251] - Starting CogView4 API server...
2025-06-20 15:14:15,332 - cogview4_api_server - INFO - [__init__:927] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-20 15:14:18,885 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:14:18,886 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:14:18,886 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:14:18,886 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:14:18,902 - cogview4_api_server - INFO - [__init__:951] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-20 15:14:18,904 - cogview4_api_server - INFO - [__init__:964] - Started worker process 0 using spawn method
2025-06-20 15:14:18,905 - cogview4_api_server - INFO - [__init__:964] - Started worker process 1 using spawn method
2025-06-20 15:14:18,905 - cogview4_api_server - INFO - [__init__:966] - Worker pool initialized with 2 workers
2025-06-20 15:14:18,905 - cogview4_api_server - INFO - [__init__:976] - Prompt batching enabled - timeout checker started
2025-06-20 15:14:18,905 - cogview4_api_server - INFO - [_monitor_worker_readiness:1208] - Starting worker readiness monitoring...
2025-06-20 15:14:18,906 - cogview4_api_server - INFO - [__init__:984] - Worker readiness monitor started
2025-06-20 15:14:18,906 - cogview4_api_server - INFO - [lifespan:1254] - Worker pool initialized successfully!
2025-06-20 15:14:22,534 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:14:22,534 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:14:22,534 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:14:22,534 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:14:22,535 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:14:22,535 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:14:22,535 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:14:22,535 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:14:22,542 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:14:22,542 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:14:22,542 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:14:22,542 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:14:22,542 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:14:22,542 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:14:22,542 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:14:22,543 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:14:22,550 - worker_0 - INFO - [worker_process:211] - Worker 0 starting up...
2025-06-20 15:14:22,550 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-20 15:14:22,551 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-20 15:14:22,551 - worker_1 - INFO - [worker_process:211] - Worker 1 starting up...
2025-06-20 15:14:22,551 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-20 15:14:22,659 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-20 15:14:22,659 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-20 15:14:25,553 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-20 15:14:25,675 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-20 15:14:25,675 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-20 15:15:12,842 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-20 15:15:12,844 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-20 15:15:12,844 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-20 15:15:12,969 - cogview4_api_server - INFO - [_monitor_worker_readiness:1224] - Worker loading progress: 1/2 workers ready
2025-06-20 15:15:17,251 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-20 15:15:17,253 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-20 15:15:17,253 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-20 15:15:18,976 - cogview4_api_server - INFO - [_monitor_worker_readiness:1217] - All workers have loaded models - displaying ready banner
2025-06-20 15:15:18,976 - cogview4_api_server - INFO - [display_ready_banner:89] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-20 15:15:18,976 - cogview4_api_server - INFO - [display_ready_banner:90] -  2 workers ready to process requests
2025-06-20 15:15:59,759 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=30
2025-06-20 15:15:59,760 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-20 15:15:59,764 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-20 15:16:00,356 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-20 15:16:00,356 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-20 15:16:00,356 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-20 15:16:00,356 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-20 15:16:00,364 - Worker0 - INFO - Worker 0: Processing batched request b6f72bae with 1 prompts, stream=True
2025-06-20 15:16:24,370 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 24.61s
2025-06-20 15:16:49,239 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=30
2025-06-20 15:16:49,239 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-20 15:16:49,240 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-20 15:16:49,830 - Worker1 - INFO - Worker 1: Processing batched request 9e943d50 with 1 prompts, stream=True
2025-06-20 15:17:13,609 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 24.37s
2025-06-20 15:17:56,440 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, steps=30
2025-06-20 15:17:56,440 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-20 15:17:56,441 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-20 15:17:57,019 - Worker0 - INFO - Worker 0: Processing batched request e12d6a1d with 1 prompts, stream=True
2025-06-20 15:19:17,467 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 81.03s
2025-06-20 15:24:16,699 - cogview4_api_server - INFO - [lifespan:1261] - Shutting down CogView4 API server...
2025-06-20 15:24:16,699 - cogview4_api_server - INFO - [shutdown:1184] - Shutting down worker pool...
2025-06-20 15:24:17,488 - cogview4_api_server - INFO - [shutdown:1204] - Worker pool shutdown complete
2025-06-20 15:24:17,489 - cogview4_api_server - INFO - [lifespan:1264] - Worker pool shut down successfully
2025-06-23 10:50:20,756 - __main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 10:50:20,756 - __main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 10:50:20,756 - __main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 10:50:20,756 - __main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 10:50:20,777 - __main__ - INFO - [<module>:1538] - Starting CogView4 Image Generation API Server with persistent worker pool...
2025-06-23 10:50:20,794 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 10:50:20,794 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 10:50:20,794 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 10:50:20,794 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 10:50:20,800 - cogview4_api_server - INFO - [lifespan:1251] - Starting CogView4 API server...
2025-06-23 10:50:20,800 - cogview4_api_server - INFO - [__init__:927] - Set multiprocessing start method to 'spawn' for CUDA compatibility
2025-06-23 10:50:24,393 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 10:50:24,394 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 10:50:24,394 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 10:50:24,394 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 10:50:24,411 - cogview4_api_server - INFO - [__init__:951] - Initializing worker pool with 2 workers using 'spawn' method
2025-06-23 10:50:24,413 - cogview4_api_server - INFO - [__init__:964] - Started worker process 0 using spawn method
2025-06-23 10:50:24,414 - cogview4_api_server - INFO - [__init__:964] - Started worker process 1 using spawn method
2025-06-23 10:50:24,414 - cogview4_api_server - INFO - [__init__:966] - Worker pool initialized with 2 workers
2025-06-23 10:50:24,414 - cogview4_api_server - INFO - [__init__:976] - Prompt batching enabled - timeout checker started
2025-06-23 10:50:24,414 - cogview4_api_server - INFO - [_monitor_worker_readiness:1208] - Starting worker readiness monitoring...
2025-06-23 10:50:24,415 - cogview4_api_server - INFO - [__init__:984] - Worker readiness monitor started
2025-06-23 10:50:24,415 - cogview4_api_server - INFO - [lifespan:1254] - Worker pool initialized successfully!
2025-06-23 10:50:28,011 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 10:50:28,011 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 10:50:28,011 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 10:50:28,011 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 10:50:28,019 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 10:50:28,019 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 10:50:28,019 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 10:50:28,019 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 10:50:28,027 - worker_0 - INFO - [worker_process:211] - Worker 0 starting up...
2025-06-23 10:50:28,028 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 10:50:28,028 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 10:50:28,079 - __mp_main__ - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 10:50:28,079 - __mp_main__ - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 10:50:28,079 - __mp_main__ - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 10:50:28,079 - __mp_main__ - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 10:50:28,086 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 10:50:28,086 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 10:50:28,087 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 10:50:28,087 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 10:50:28,095 - worker_1 - INFO - [worker_process:211] - Worker 1 starting up...
2025-06-23 10:50:28,095 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 10:50:28,142 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 10:50:28,142 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 10:50:31,097 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 10:50:31,222 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 10:50:31,223 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 10:51:20,350 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 10:51:20,352 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 10:51:20,352 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 10:51:20,479 - cogview4_api_server - INFO - [_monitor_worker_readiness:1224] - Worker loading progress: 1/2 workers ready
2025-06-23 10:51:21,190 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 10:51:21,191 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 10:51:21,192 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 10:51:22,482 - cogview4_api_server - INFO - [_monitor_worker_readiness:1217] - All workers have loaded models - displaying ready banner
2025-06-23 10:51:22,482 - cogview4_api_server - INFO - [display_ready_banner:89] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 10:51:22,482 - cogview4_api_server - INFO - [display_ready_banner:90] -  2 workers ready to process requests
2025-06-23 11:14:04,977 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-23 11:14:04,977 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:14:04,982 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:14:05,527 - cogview4_api_server - INFO - [<module>:55] - Starting CogView4 API with log level: INFO
2025-06-23 11:14:05,527 - cogview4_api_server - INFO - [<module>:56] - Log file: cogview4_api.log
2025-06-23 11:14:05,527 - cogview4_api_server - INFO - [<module>:57] - Number of worker processes: 2
2025-06-23 11:14:05,527 - cogview4_api_server - INFO - [<module>:58] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 11:14:05,536 - Worker1 - INFO - Worker 1: Processing batched request 6f1c8bf7 with 1 prompts, stream=True
2025-06-23 11:14:25,384 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-23 11:14:25,384 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:14:25,385 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:14:25,963 - Worker0 - INFO - Worker 0: Processing batched request db362aa1 with 1 prompts, stream=True
2025-06-23 11:14:50,334 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 24.95s
2025-06-23 11:15:27,250 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 82.27s
2025-06-23 11:15:31,232 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-23 11:15:31,233 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:15:31,233 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:15:31,749 - Worker1 - INFO - Worker 1: Processing batched request 33b9faa5 with 1 prompts, stream=True
2025-06-23 11:15:55,594 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 24.36s
2025-06-23 11:18:35,223 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=20
2025-06-23 11:18:35,223 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:18:35,224 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:18:35,779 - Worker0 - INFO - Worker 0: Processing batched request ffb0ac66 with 1 prompts, stream=True
2025-06-23 11:18:51,754 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 16.53s
2025-06-23 11:19:12,789 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=25
2025-06-23 11:19:12,790 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:19:12,790 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:19:13,329 - Worker1 - INFO - Worker 1: Processing batched request 17766419 with 1 prompts, stream=True
2025-06-23 11:19:32,940 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 20.15s
2025-06-23 11:19:48,096 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=30
2025-06-23 11:19:48,097 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:19:48,098 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:19:48,676 - Worker0 - INFO - Worker 0: Processing batched request 7cfe90b6 with 1 prompts, stream=True
2025-06-23 11:20:12,131 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 24.03s
2025-06-23 11:20:21,004 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=30
2025-06-23 11:20:21,004 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:20:21,005 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:20:21,518 - Worker1 - INFO - Worker 1: Processing batched request 10cc044f with 1 prompts, stream=True
2025-06-23 11:20:44,249 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 23.24s
2025-06-23 11:20:56,677 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=30
2025-06-23 11:20:56,677 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:20:56,678 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:20:57,261 - Worker1 - INFO - Worker 1: Processing batched request e21d8d97 with 1 prompts, stream=True
2025-06-23 11:21:20,070 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 23.39s
2025-06-23 11:21:31,609 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=30
2025-06-23 11:21:31,609 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:21:31,609 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:21:32,204 - Worker1 - INFO - Worker 1: Processing batched request 8cf11cf7 with 1 prompts, stream=True
2025-06-23 11:21:55,097 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 23.49s
2025-06-23 11:22:05,316 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=30
2025-06-23 11:22:05,316 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:22:05,317 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:22:05,849 - Worker1 - INFO - Worker 1: Processing batched request 23c1e3c3 with 1 prompts, stream=True
2025-06-23 11:22:28,804 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 23.49s
2025-06-23 11:22:38,025 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=30
2025-06-23 11:22:38,025 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:22:38,026 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:22:38,593 - Worker1 - INFO - Worker 1: Processing batched request 06270bc7 with 1 prompts, stream=True
2025-06-23 11:23:01,501 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 23.48s
2025-06-23 11:23:13,043 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='32K...', stream=True, steps=30
2025-06-23 11:23:13,043 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:23:13,044 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:23:13,640 - Worker0 - INFO - Worker 0: Processing batched request b64422da with 1 prompts, stream=True
2025-06-23 11:23:36,584 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 23.54s
2025-06-23 11:24:37,788 - cogview4_api_server - INFO - [create_image:1361] - Received image generation request: prompt='...', stream=True, steps=30
2025-06-23 11:24:37,789 - cogview4_api_server - INFO - [create_image:1378] - Starting streaming response
2025-06-23 11:24:37,789 - cogview4_api_server - INFO - [generate_sse_stream:1291] - Starting SSE stream generation
2025-06-23 11:24:38,352 - Worker1 - INFO - Worker 1: Processing batched request ced20d7c with 1 prompts, stream=True
2025-06-23 11:25:01,654 - cogview4_api_server - INFO - [generate_sse_stream:1336] - SSE streaming completed in 23.86s
2025-06-23 15:38:10,374 - cogview4_api_server - INFO - [lifespan:1261] - Shutting down CogView4 API server...
2025-06-23 15:38:10,376 - cogview4_api_server - INFO - [shutdown:1184] - Shutting down worker pool...
2025-06-23 15:38:11,223 - cogview4_api_server - INFO - [shutdown:1204] - Worker pool shutdown complete
2025-06-23 15:38:11,223 - cogview4_api_server - INFO - [lifespan:1264] - Worker pool shut down successfully
2025-06-23 17:42:56,318 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:42:56,318 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:42:56,318 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:42:56,318 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:42:56,318 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:42:56,318 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:42:56,318 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:42:56,318 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:42:59,740 - __main__ - INFO - [<module>:221] - Starting Uvicorn server for CogView4 API...
2025-06-23 17:42:59,759 - main - INFO - [lifespan:45] - Starting CogView4 API server...
2025-06-23 17:42:59,759 - main - INFO - [get_worker_pool:34] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 17:42:59,759 - processing - INFO - [__init__:110] - Set multiprocessing start method to 'spawn'
2025-06-23 17:43:00,028 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:43:00,028 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:43:00,028 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:43:00,028 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:43:00,028 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:43:00,028 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:43:00,028 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:43:00,028 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:43:03,322 - processing - INFO - [__init__:124] - Initializing worker pool with 2 workers
2025-06-23 17:43:03,324 - processing - INFO - [__init__:133] - Worker pool initialized with 2 workers
2025-06-23 17:43:03,324 - processing - INFO - [__init__:140] - Prompt batching enabled - timeout checker started
2025-06-23 17:43:03,325 - processing - INFO - [_monitor_worker_readiness:247] - Starting worker readiness monitoring...
2025-06-23 17:43:03,325 - processing - INFO - [__init__:147] - Worker readiness monitor started
2025-06-23 17:43:03,325 - main - INFO - [lifespan:48] - Worker pool initialization process started.
2025-06-23 17:43:03,586 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:43:03,586 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:43:03,586 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:43:03,586 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:43:03,586 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:43:03,586 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:43:03,586 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:43:03,586 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:43:03,586 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:43:03,586 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:43:03,586 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:43:03,586 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:43:03,586 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:43:03,586 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:43:03,586 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:43:03,587 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:43:07,043 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 17:43:07,043 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 17:43:07,056 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 17:43:07,154 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 17:43:07,154 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 17:43:10,059 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 17:43:10,204 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 17:43:10,204 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 17:43:41,934 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 17:43:41,936 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 17:43:41,936 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 17:43:43,370 - processing - INFO - [_monitor_worker_readiness:260] - Worker loading progress: 1/2 workers ready
2025-06-23 17:43:43,492 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 17:43:43,494 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 17:43:43,494 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 17:43:45,373 - processing - INFO - [_monitor_worker_readiness:254] - All workers have loaded models - displaying ready banner
2025-06-23 17:43:45,374 - config - INFO - [display_ready_banner:33] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 17:43:45,374 - config - INFO - [display_ready_banner:34] -  2 workers ready to process requests
2025-06-23 17:44:17,801 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 17:44:17,807 - main - INFO - [generate_sse_stream:84] - Starting SSE stream generation
2025-06-23 17:44:18,340 - Worker1 - INFO - Worker 1: Processing batched request 2170b37a with 1 prompts, stream=True
2025-06-23 17:44:38,380 - main - INFO - [generate_sse_stream:116] - SSE streaming completed in 20.57s
2025-06-23 17:45:04,472 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 17:45:04,473 - main - INFO - [generate_sse_stream:84] - Starting SSE stream generation
2025-06-23 17:45:05,005 - Worker1 - INFO - Worker 1: Processing batched request f97d0dda with 1 prompts, stream=True
2025-06-23 17:46:04,553 - main - INFO - [generate_sse_stream:116] - SSE streaming completed in 60.08s
2025-06-23 17:48:43,186 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:48:43,187 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:48:43,187 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:48:43,187 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:48:43,187 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:48:43,187 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:48:43,187 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:48:43,187 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:48:46,488 - __main__ - INFO - [<module>:221] - Starting Uvicorn server for CogView4 API...
2025-06-23 17:48:46,505 - main - INFO - [lifespan:45] - Starting CogView4 API server...
2025-06-23 17:48:46,505 - main - INFO - [get_worker_pool:34] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 17:48:46,505 - processing - INFO - [__init__:110] - Set multiprocessing start method to 'spawn'
2025-06-23 17:48:46,774 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:48:46,774 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:48:46,774 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:48:46,774 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:48:46,774 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:48:46,774 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:48:46,774 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:48:46,774 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:48:50,067 - processing - INFO - [__init__:124] - Initializing worker pool with 2 workers
2025-06-23 17:48:50,069 - processing - INFO - [__init__:133] - Worker pool initialized with 2 workers
2025-06-23 17:48:50,070 - processing - INFO - [__init__:140] - Prompt batching enabled - timeout checker started
2025-06-23 17:48:50,070 - processing - INFO - [_monitor_worker_readiness:247] - Starting worker readiness monitoring...
2025-06-23 17:48:50,070 - processing - INFO - [__init__:147] - Worker readiness monitor started
2025-06-23 17:48:50,071 - main - INFO - [lifespan:48] - Worker pool initialization process started.
2025-06-23 17:48:50,071 - main - INFO - [lifespan:54] - Shutting down CogView4 API server...
2025-06-23 17:48:50,071 - processing - INFO - [shutdown:231] - Shutting down worker pool...
2025-06-23 17:48:50,331 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:48:50,331 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:48:50,331 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:48:50,331 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:48:50,331 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:48:50,332 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:48:50,332 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:48:50,332 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:48:50,332 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 17:48:50,332 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 17:48:50,332 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 17:48:50,332 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 17:48:50,332 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 17:48:50,332 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 17:48:50,332 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 17:48:50,332 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 17:48:52,298 - processing - INFO - [shutdown:244] - Worker pool shutdown complete
2025-06-23 17:48:52,298 - main - INFO - [lifespan:58] - Worker pool shut down successfully
2025-06-23 17:58:18,201 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 17:58:18,202 - main - INFO - [generate_sse_stream:84] - Starting SSE stream generation
2025-06-23 17:58:18,732 - Worker1 - INFO - Worker 1: Processing batched request 3d1aa44a with 1 prompts, stream=True
2025-06-23 17:59:20,207 - main - INFO - [generate_sse_stream:116] - SSE streaming completed in 62.01s
2025-06-23 17:59:31,362 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=False
2025-06-23 17:59:31,362 - main - ERROR - [create_image:159] - Request failed: object async_generator can't be used in 'await' expression
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 152, in create_image
    images_b64 = await pool.submit_request(stream=False, **request_params)
TypeError: object async_generator can't be used in 'await' expression
2025-06-23 18:01:30,345 - main - INFO - [lifespan:54] - Shutting down CogView4 API server...
2025-06-23 18:01:30,346 - processing - INFO - [shutdown:231] - Shutting down worker pool...
2025-06-23 18:01:31,111 - processing - INFO - [shutdown:244] - Worker pool shutdown complete
2025-06-23 18:01:31,111 - main - INFO - [lifespan:58] - Worker pool shut down successfully
2025-06-23 18:01:33,964 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:01:33,964 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:01:33,965 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:01:33,965 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:01:33,965 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:01:33,965 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:01:33,965 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:01:33,965 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:02:35,173 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:02:35,174 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:02:35,174 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:02:35,174 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:02:35,174 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:02:35,174 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:02:35,174 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:02:35,174 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:02:38,618 - __main__ - INFO - [<module>:221] - Starting Uvicorn server for CogView4 API...
2025-06-23 18:02:38,637 - main - INFO - [lifespan:45] - Starting CogView4 API server...
2025-06-23 18:02:38,637 - main - INFO - [get_worker_pool:34] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 18:02:38,637 - processing - INFO - [__init__:110] - Set multiprocessing start method to 'spawn'
2025-06-23 18:02:38,904 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:02:38,904 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:02:38,904 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:02:38,904 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:02:38,904 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:02:38,904 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:02:38,904 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:02:38,904 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:02:42,234 - processing - INFO - [__init__:124] - Initializing worker pool with 2 workers
2025-06-23 18:02:42,237 - processing - INFO - [__init__:133] - Worker pool initialized with 2 workers
2025-06-23 18:02:42,237 - processing - INFO - [__init__:140] - Prompt batching enabled - timeout checker started
2025-06-23 18:02:42,238 - processing - INFO - [_monitor_worker_readiness:273] - Starting worker readiness monitoring...
2025-06-23 18:02:42,238 - processing - INFO - [__init__:147] - Worker readiness monitor started
2025-06-23 18:02:42,238 - main - INFO - [lifespan:48] - Worker pool initialization process started.
2025-06-23 18:02:42,498 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:02:42,498 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:02:42,498 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:02:42,498 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:02:42,498 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:02:42,498 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:02:42,498 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:02:42,498 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:02:42,500 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:02:42,500 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:02:42,500 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:02:42,500 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:02:42,500 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:02:42,500 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:02:42,500 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:02:42,500 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:02:45,822 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 18:02:45,823 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 18:02:45,840 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 18:02:45,934 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 18:02:45,935 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 18:02:48,841 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 18:02:48,987 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 18:02:48,987 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 18:03:04,125 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=False
2025-06-23 18:03:19,706 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 18:03:19,707 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 18:03:19,707 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 18:03:19,708 - Worker1 - INFO - Worker 1: Processing batched request 5eb583bf with 1 prompts, stream=False
2025-06-23 18:03:19,738 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 18:03:19,739 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 18:03:19,739 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 18:03:20,283 - processing - INFO - [_monitor_worker_readiness:280] - All workers have loaded models - displaying ready banner
2025-06-23 18:03:20,284 - config - INFO - [display_ready_banner:33] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 18:03:20,284 - config - INFO - [display_ready_banner:34] -  2 workers ready to process requests
2025-06-23 18:03:44,018 - main - INFO - [create_image:155] - Non-streaming request completed in 39.89s
2025-06-23 18:03:55,923 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 18:03:55,927 - main - INFO - [generate_sse_stream:84] - Starting SSE stream generation
2025-06-23 18:03:56,467 - Worker0 - INFO - Worker 0: Processing batched request 395cb38c with 1 prompts, stream=True
2025-06-23 18:04:29,001 - main - INFO - [generate_sse_stream:116] - SSE streaming completed in 33.07s
2025-06-23 18:11:17,598 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 18:11:17,599 - main - INFO - [generate_sse_stream:84] - Starting SSE stream generation
2025-06-23 18:11:18,159 - Worker1 - INFO - Worker 1: Processing batched request 0185e8ee with 1 prompts, stream=True
2025-06-23 18:11:55,483 - main - INFO - [generate_sse_stream:116] - SSE streaming completed in 37.88s
2025-06-23 18:12:23,262 - main - INFO - [create_image:130] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 18:12:23,263 - main - INFO - [generate_sse_stream:84] - Starting SSE stream generation
2025-06-23 18:12:23,847 - Worker0 - INFO - Worker 0: Processing batched request 6582fd2f with 1 prompts, stream=True
2025-06-23 18:13:25,944 - main - INFO - [generate_sse_stream:116] - SSE streaming completed in 62.68s
2025-06-23 18:36:28,312 - main - INFO - [lifespan:54] - Shutting down CogView4 API server...
2025-06-23 18:36:28,312 - processing - INFO - [shutdown:257] - Shutting down worker pool...
2025-06-23 18:36:29,102 - processing - INFO - [shutdown:270] - Worker pool shutdown complete
2025-06-23 18:36:29,102 - main - INFO - [lifespan:58] - Worker pool shut down successfully
2025-06-23 18:36:31,954 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:36:31,954 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:36:31,954 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:36:31,954 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:36:31,954 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:36:31,954 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:36:31,954 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:36:31,954 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:36:35,356 - __main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:36:35,371 - __main__ - INFO - [<module>:217] - Starting Uvicorn server for CogView4 API...
2025-06-23 18:36:35,389 - main - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:36:35,390 - main - INFO - [lifespan:46] - Starting CogView4 API server...
2025-06-23 18:36:35,390 - main - INFO - [get_worker_pool:35] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 18:36:35,390 - processing - INFO - [__init__:110] - Set multiprocessing start method to 'spawn'
2025-06-23 18:36:35,663 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:36:35,663 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:36:35,663 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:36:35,663 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:36:35,663 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:36:35,663 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:36:35,663 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:36:35,663 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:36:38,956 - __mp_main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:36:38,968 - processing - INFO - [__init__:124] - Initializing worker pool with 2 workers
2025-06-23 18:36:38,971 - processing - INFO - [__init__:133] - Worker pool initialized with 2 workers
2025-06-23 18:36:38,971 - processing - INFO - [__init__:140] - Prompt batching enabled - timeout checker started
2025-06-23 18:36:38,971 - processing - INFO - [_monitor_worker_readiness:273] - Starting worker readiness monitoring...
2025-06-23 18:36:38,972 - processing - INFO - [__init__:147] - Worker readiness monitor started
2025-06-23 18:36:38,972 - main - INFO - [lifespan:49] - Worker pool initialization process started.
2025-06-23 18:36:39,234 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:36:39,234 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:36:39,234 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:36:39,234 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:36:39,234 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:36:39,234 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:36:39,234 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:36:39,234 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:36:39,234 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:36:39,234 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:36:39,234 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:36:39,234 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:36:39,234 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:36:39,234 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:36:39,234 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:36:39,234 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:36:42,570 - __mp_main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:36:42,575 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 18:36:42,582 - __mp_main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:36:42,587 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 18:36:42,587 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 18:36:42,697 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 18:36:42,697 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 18:36:45,577 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 18:36:45,707 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 18:36:45,707 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 18:37:16,993 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 18:37:16,995 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 18:37:16,995 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 18:37:17,017 - processing - INFO - [_monitor_worker_readiness:286] - Worker loading progress: 1/2 workers ready
2025-06-23 18:37:17,098 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 18:37:17,100 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 18:37:17,101 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 18:37:19,020 - processing - INFO - [_monitor_worker_readiness:280] - All workers have loaded models - displaying ready banner
2025-06-23 18:37:19,020 - config - INFO - [display_ready_banner:33] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 18:37:19,021 - config - INFO - [display_ready_banner:34] -  2 workers ready to process requests
2025-06-23 18:37:32,611 - main - INFO - [create_image:138] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 18:37:32,612 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:37:33,160 - Worker1 - INFO - Worker 1: Processing batched request cf93d809 with 1 prompts, stream=True
2025-06-23 18:38:11,280 - main - INFO - [generate_sse_stream:124] - SSE streaming completed in 38.67s
2025-06-23 18:42:29,485 - main - INFO - [create_image:138] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 18:42:29,486 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:42:30,060 - Worker0 - INFO - Worker 0: Processing batched request e3de4f7f with 1 prompts, stream=True
2025-06-23 18:43:32,559 - main - INFO - [generate_sse_stream:124] - SSE streaming completed in 63.07s
2025-06-23 18:43:53,735 - main - INFO - [create_image:138] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True
2025-06-23 18:43:53,736 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:43:54,269 - Worker1 - INFO - Worker 1: Processing batched request 4e9dbeee with 1 prompts, stream=True
2025-06-23 18:45:45,073 - main - INFO - [generate_sse_stream:124] - SSE streaming completed in 111.34s
2025-06-23 18:54:26,842 - main - INFO - [lifespan:55] - Shutting down CogView4 API server...
2025-06-23 18:54:26,842 - processing - INFO - [shutdown:257] - Shutting down worker pool...
2025-06-23 18:54:27,570 - processing - INFO - [shutdown:270] - Worker pool shutdown complete
2025-06-23 18:54:27,572 - main - INFO - [lifespan:59] - Worker pool shut down successfully
2025-06-23 18:54:32,038 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:54:32,038 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:54:32,038 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:54:32,038 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:54:32,038 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:54:32,038 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:54:32,038 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:54:32,038 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:54:35,458 - __main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:54:35,474 - __main__ - INFO - [<module>:224] - Starting Uvicorn server for CogView4 API...
2025-06-23 18:54:35,491 - main - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:54:35,492 - main - INFO - [lifespan:46] - Starting CogView4 API server...
2025-06-23 18:54:35,492 - main - INFO - [get_worker_pool:35] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 18:54:35,492 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 18:54:35,761 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:54:35,761 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:54:35,761 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:54:35,761 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:54:35,761 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:54:35,761 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:54:35,761 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:54:35,762 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:54:39,075 - __mp_main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:54:39,086 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 18:54:39,089 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 18:54:39,089 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 18:54:39,089 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 18:54:39,090 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 18:54:39,090 - main - INFO - [lifespan:49] - Worker pool initialization process started.
2025-06-23 18:54:39,350 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:54:39,350 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:54:39,350 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:54:39,350 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:54:39,350 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:54:39,350 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:54:39,350 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:54:39,350 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:54:39,351 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 18:54:39,352 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 18:54:39,352 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 18:54:39,352 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 18:54:39,352 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 18:54:39,352 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 18:54:39,352 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 18:54:39,352 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 18:54:42,672 - __mp_main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:54:42,676 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 18:54:42,686 - __mp_main__ - INFO - [<module>:83] - Static files mounted successfully
2025-06-23 18:54:42,691 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 18:54:42,691 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 18:54:42,802 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 18:54:42,802 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 18:54:45,679 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 18:54:45,822 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 18:54:45,822 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 18:55:14,333 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 18:55:14,335 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 18:55:14,335 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 18:55:15,134 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 18:55:16,909 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 18:55:16,911 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 18:55:16,911 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 18:55:17,137 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 18:55:17,138 - config - INFO - [display_ready_banner:33] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 18:55:17,138 - config - INFO - [display_ready_banner:34] -  2 workers ready to process requests
2025-06-23 18:55:21,517 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=1073668504
2025-06-23 18:55:21,517 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:55:22,058 - Worker0 - INFO - Worker 0: Processing batched request 571ee94d with 1 prompts, stream=True
2025-06-23 18:56:00,536 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 39.02s
2025-06-23 18:56:36,454 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=1073668504
2025-06-23 18:56:36,455 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:56:36,958 - Worker0 - INFO - Worker 0: Processing batched request 544fdef3 with 1 prompts, stream=True
2025-06-23 18:57:39,481 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 63.03s
2025-06-23 18:58:00,307 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=1073668504
2025-06-23 18:58:00,307 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:58:00,868 - Worker1 - INFO - Worker 1: Processing batched request 50c48598 with 1 prompts, stream=True
2025-06-23 18:58:17,468 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 17.16s
2025-06-23 18:58:27,391 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=1073668504
2025-06-23 18:58:27,392 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:58:27,905 - Worker0 - INFO - Worker 0: Processing batched request 52a943d7 with 1 prompts, stream=True
2025-06-23 18:58:43,819 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 16.43s
2025-06-23 18:58:57,794 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=668927615
2025-06-23 18:58:57,795 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:58:58,346 - Worker1 - INFO - Worker 1: Processing batched request d29c4149 with 1 prompts, stream=True
2025-06-23 18:59:14,062 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 16.27s
2025-06-23 18:59:45,088 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=668927615
2025-06-23 18:59:45,089 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 18:59:45,609 - Worker0 - INFO - Worker 0: Processing batched request ccdade4a with 1 prompts, stream=True
2025-06-23 19:00:01,415 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 16.33s
2025-06-23 19:00:25,514 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=668927615
2025-06-23 19:00:25,515 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 19:00:26,062 - Worker1 - INFO - Worker 1: Processing batched request 51cd3e1f with 1 prompts, stream=True
2025-06-23 19:00:51,735 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 26.22s
2025-06-23 19:01:15,734 - main - INFO - [create_image:140] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=668927615
2025-06-23 19:01:15,735 - main - INFO - [generate_sse_stream:92] - Starting SSE stream generation
2025-06-23 19:01:16,330 - Worker0 - INFO - Worker 0: Processing batched request c75e016e with 1 prompts, stream=True
2025-06-23 19:01:42,052 - main - INFO - [generate_sse_stream:126] - SSE streaming completed in 26.32s
2025-06-23 19:09:52,235 - main - INFO - [lifespan:55] - Shutting down CogView4 API server...
2025-06-23 19:09:52,236 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 19:09:53,002 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 19:09:53,003 - main - INFO - [lifespan:59] - Worker pool shut down successfully
2025-06-23 19:22:06,951 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:22:06,951 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:22:06,951 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:22:06,951 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:22:06,951 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:22:06,951 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:22:06,951 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:22:06,951 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:22:29,976 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:22:29,976 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:22:29,976 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:22:29,976 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:22:29,976 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:22:29,976 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:22:29,976 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:22:29,976 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:22:33,619 - __main__ - INFO - [<module>:84] - Static files mounted successfully
2025-06-23 19:22:33,631 - __main__ - INFO - [<module>:266] - Starting Uvicorn server for CogView4 API...
2025-06-23 19:22:33,649 - main - INFO - [<module>:84] - Static files mounted successfully
2025-06-23 19:22:33,651 - main - INFO - [lifespan:47] - Starting CogView4 API server...
2025-06-23 19:22:33,651 - main - INFO - [get_worker_pool:36] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 19:22:33,651 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 19:22:33,919 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:22:33,919 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:22:33,919 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:22:33,919 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:22:33,919 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:22:33,919 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:22:33,919 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:22:33,919 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:22:37,459 - __mp_main__ - INFO - [<module>:84] - Static files mounted successfully
2025-06-23 19:22:37,471 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 19:22:37,474 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 19:22:37,474 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 19:22:37,475 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 19:22:37,475 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 19:22:37,475 - main - INFO - [lifespan:50] - Worker pool initialization process started.
2025-06-23 19:22:37,736 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:22:37,736 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:22:37,737 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:22:37,737 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:22:37,737 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:22:37,737 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:22:37,737 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:22:37,737 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:22:37,737 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:22:37,738 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:22:37,738 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:22:37,738 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:22:37,738 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:22:37,738 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:22:37,738 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:22:37,738 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:22:41,303 - __mp_main__ - INFO - [<module>:84] - Static files mounted successfully
2025-06-23 19:22:41,307 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 19:22:41,320 - __mp_main__ - INFO - [<module>:84] - Static files mounted successfully
2025-06-23 19:22:41,325 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 19:22:41,325 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 19:22:41,441 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 19:22:41,442 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 19:22:44,311 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 19:22:44,468 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 19:22:44,469 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 19:23:02,264 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='a cat sitting on a chair...'
2025-06-23 19:23:07,133 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:23:07,139 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 4.87s
2025-06-23 19:23:07,144 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='...'
2025-06-23 19:23:10,624 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:23:10,626 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 3.48s
2025-06-23 19:23:10,631 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='An anime girl with blue hair in a magical forest w...'
2025-06-23 19:23:14,481 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:23:14,482 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 3.85s
2025-06-23 19:23:14,486 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='...'
2025-06-23 19:23:16,361 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 19:23:16,364 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 19:23:16,364 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 19:23:17,526 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 19:23:18,906 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:23:18,908 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 4.42s
2025-06-23 19:23:19,299 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 19:23:19,301 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 19:23:19,301 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 19:23:19,529 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 19:23:19,530 - config - INFO - [display_ready_banner:38] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 19:23:19,530 - config - INFO - [display_ready_banner:39] -  2 workers ready to process requests
2025-06-23 19:23:43,225 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='a beautiful sunset...'
2025-06-23 19:23:46,670 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:23:46,672 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 3.45s
2025-06-23 19:28:59,433 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='A beautiful landscape with mountains and a lake at...'
2025-06-23 19:29:02,714 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:29:02,716 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 3.28s
2025-06-23 19:29:25,682 - main - INFO - [create_image:141] - Received image generation request: prompt='This image captures a breathtaking landscape at su...', stream=True, seed=None
2025-06-23 19:29:25,683 - main - INFO - [generate_sse_stream:93] - Starting SSE stream generation
2025-06-23 19:29:26,229 - Worker1 - INFO - Worker 1: Processing batched request b4b814a3 with 1 prompts, stream=True
2025-06-23 19:29:42,829 - main - INFO - [generate_sse_stream:127] - SSE streaming completed in 17.15s
2025-06-23 19:33:22,790 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='A beautiful landscape with mountains and a lake at...'
2025-06-23 19:33:26,119 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:33:26,120 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 3.33s
2025-06-23 19:35:07,160 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='A beautiful landscape with mountains and a lake at...'
2025-06-23 19:35:10,679 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:35:10,681 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 3.52s
2025-06-23 19:35:31,043 - main - INFO - [create_image:141] - Received image generation request: prompt='This image captures a breathtaking landscape at su...', stream=True, seed=None
2025-06-23 19:35:31,043 - main - INFO - [generate_sse_stream:93] - Starting SSE stream generation
2025-06-23 19:35:31,609 - Worker1 - INFO - Worker 1: Processing batched request 128e4a4c with 1 prompts, stream=True
2025-06-23 19:35:47,552 - main - INFO - [generate_sse_stream:127] - SSE streaming completed in 16.51s
2025-06-23 19:35:59,528 - main - INFO - [optimize_prompt:221] - Received prompt optimization request: prompt='A beautiful landscape with mountains and a lake at...'
2025-06-23 19:36:02,801 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 19:36:02,803 - main - INFO - [optimize_prompt:239] - Prompt optimization completed in 3.27s
2025-06-23 19:36:38,881 - main - INFO - [create_image:141] - Received image generation request: prompt='This image captures a breathtaking landscape at su...', stream=True, seed=None
2025-06-23 19:36:38,882 - main - INFO - [generate_sse_stream:93] - Starting SSE stream generation
2025-06-23 19:36:39,399 - Worker1 - INFO - Worker 1: Processing batched request c5c8ed38 with 1 prompts, stream=True
2025-06-23 19:36:55,185 - main - INFO - [generate_sse_stream:127] - SSE streaming completed in 16.30s
2025-06-23 19:47:27,546 - main - INFO - [lifespan:56] - Shutting down CogView4 API server...
2025-06-23 19:47:27,547 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 19:47:28,372 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 19:47:28,372 - main - INFO - [lifespan:60] - Worker pool shut down successfully
2025-06-23 19:47:32,237 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:47:32,237 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:47:32,238 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:47:32,238 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:47:32,238 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:47:32,238 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:47:32,238 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:47:32,238 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:47:35,864 - __main__ - INFO - [<module>:85] - Static files mounted successfully
2025-06-23 19:47:35,875 - __main__ - INFO - [<module>:359] - Starting Uvicorn server for CogView4 API...
2025-06-23 19:47:35,894 - main - INFO - [<module>:85] - Static files mounted successfully
2025-06-23 19:47:35,896 - main - INFO - [lifespan:48] - Starting CogView4 API server...
2025-06-23 19:47:35,896 - main - INFO - [get_worker_pool:37] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 19:47:35,896 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 19:47:36,166 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:47:36,167 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:47:36,167 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:47:36,167 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:47:36,167 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:47:36,167 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:47:36,167 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:47:36,167 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:47:39,695 - __mp_main__ - INFO - [<module>:85] - Static files mounted successfully
2025-06-23 19:47:39,707 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 19:47:39,709 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 19:47:39,710 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 19:47:39,710 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 19:47:39,710 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 19:47:39,711 - main - INFO - [lifespan:51] - Worker pool initialization process started.
2025-06-23 19:47:39,973 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:47:39,973 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:47:39,973 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:47:39,973 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:47:39,973 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:47:39,973 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:47:39,973 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:47:39,973 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:47:39,973 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 19:47:39,974 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 19:47:39,974 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 19:47:39,974 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 19:47:39,974 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 19:47:39,974 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 19:47:39,974 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 19:47:39,974 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 19:47:43,576 - __mp_main__ - INFO - [<module>:85] - Static files mounted successfully
2025-06-23 19:47:43,578 - __mp_main__ - INFO - [<module>:85] - Static files mounted successfully
2025-06-23 19:47:43,581 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 19:47:43,581 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 19:47:43,583 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 19:47:43,690 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 19:47:43,690 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 19:47:46,587 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 19:47:46,715 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 19:47:46,715 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 19:48:16,338 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 19:48:16,339 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 19:48:16,339 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 19:48:17,756 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 19:48:19,443 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 19:48:19,445 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 19:48:19,445 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 19:48:19,759 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 19:48:19,759 - config - INFO - [display_ready_banner:38] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 19:48:19,759 - config - INFO - [display_ready_banner:39] -  2 workers ready to process requests
2025-06-23 19:49:57,740 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:50:24,304 - main - INFO - [create_image:142] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=12345
2025-06-23 19:50:24,305 - main - INFO - [generate_sse_stream:94] - Starting SSE stream generation
2025-06-23 19:50:24,852 - Worker0 - INFO - Worker 0: Processing batched request 0b3af1ca with 1 prompts, stream=True
2025-06-23 19:50:41,393 - main - INFO - [generate_sse_stream:128] - SSE streaming completed in 17.09s
2025-06-23 19:50:52,067 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:23,899 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:25,168 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:26,141 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:28,707 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:29,688 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:30,456 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:30,652 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:30,850 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:31,039 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 19:59:31,237 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 20:03:58,123 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 20:04:38,340 - main - INFO - [create_image:142] - Received image generation request: prompt='Cute and adorable fluffy cute creature fantasy, dr...', stream=True, seed=None
2025-06-23 20:04:38,341 - main - INFO - [generate_sse_stream:94] - Starting SSE stream generation
2025-06-23 20:04:38,868 - Worker0 - INFO - Worker 0: Processing batched request 9ddb39a8 with 1 prompts, stream=True
2025-06-23 20:04:54,798 - main - INFO - [generate_sse_stream:128] - SSE streaming completed in 16.46s
2025-06-23 20:06:31,993 - main - INFO - [optimize_prompt:222] - Received prompt optimization request: prompt='Cute and adorable fluffy cute creature fantasy, dr...'
2025-06-23 20:06:35,758 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 20:06:35,763 - main - INFO - [optimize_prompt:240] - Prompt optimization completed in 3.77s
2025-06-23 20:06:39,621 - main - INFO - [create_image:142] - Received image generation request: prompt='In this enchanting and dreamlike fantasy scene, a ...', stream=True, seed=None
2025-06-23 20:06:39,622 - main - INFO - [generate_sse_stream:94] - Starting SSE stream generation
2025-06-23 20:06:40,176 - Worker1 - INFO - Worker 1: Processing batched request 554eba0a with 1 prompts, stream=True
2025-06-23 20:06:56,931 - main - INFO - [generate_sse_stream:128] - SSE streaming completed in 17.31s
2025-06-23 20:07:26,349 - main - INFO - [optimize_prompt:222] - Received prompt optimization request: prompt='In this enchanting and dreamlike fantasy scene, a ...'
2025-06-23 20:07:30,605 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 20:07:30,607 - main - INFO - [optimize_prompt:240] - Prompt optimization completed in 4.26s
2025-06-23 20:14:06,579 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 20:14:22,568 - main - INFO - [get_gallery:262] - Received gallery request
2025-06-23 20:22:05,808 - main - INFO - [lifespan:57] - Shutting down CogView4 API server...
2025-06-23 20:22:05,810 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 20:22:06,665 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 20:22:06,665 - main - INFO - [lifespan:61] - Worker pool shut down successfully
2025-06-23 20:22:11,151 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:22:11,151 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:22:11,151 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:22:11,151 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:22:11,151 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:22:11,152 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:22:11,152 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:22:11,152 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:22:14,776 - __main__ - INFO - [<module>:86] - Static files mounted successfully
2025-06-23 20:22:14,788 - __main__ - INFO - [<module>:338] - Starting Uvicorn server for CogView4 API...
2025-06-23 20:22:14,806 - main - INFO - [<module>:86] - Static files mounted successfully
2025-06-23 20:22:14,808 - main - INFO - [lifespan:49] - Starting CogView4 API server...
2025-06-23 20:22:14,808 - main - INFO - [get_worker_pool:38] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 20:22:14,808 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 20:22:15,077 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:22:15,077 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:22:15,077 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:22:15,077 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:22:15,077 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:22:15,077 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:22:15,077 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:22:15,077 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:22:18,587 - __mp_main__ - INFO - [<module>:86] - Static files mounted successfully
2025-06-23 20:22:18,599 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 20:22:18,601 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 20:22:18,601 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 20:22:18,601 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 20:22:18,602 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 20:22:18,602 - main - INFO - [lifespan:52] - Worker pool initialization process started.
2025-06-23 20:22:18,862 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:22:18,863 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:22:18,863 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:22:18,863 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:22:18,863 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:22:18,863 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:22:18,863 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:22:18,863 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:22:18,864 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:22:18,864 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:22:18,864 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:22:18,864 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:22:18,864 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:22:18,864 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:22:18,864 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:22:18,864 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:22:22,439 - __mp_main__ - INFO - [<module>:86] - Static files mounted successfully
2025-06-23 20:22:22,441 - __mp_main__ - INFO - [<module>:86] - Static files mounted successfully
2025-06-23 20:22:22,445 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 20:22:22,445 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 20:22:22,446 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 20:22:22,552 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 20:22:22,552 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 20:22:25,449 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 20:22:25,608 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 20:22:25,608 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 20:22:40,271 - main - INFO - [get_gallery:263] - Received gallery request
2025-06-23 20:22:40,272 - main - INFO - [get_gallery:309] - Loaded 6 images from gallery JSON
2025-06-23 20:22:52,802 - main - INFO - [get_gallery:263] - Received gallery request
2025-06-23 20:22:52,803 - main - INFO - [get_gallery:309] - Loaded 6 images from gallery JSON
2025-06-23 20:22:56,153 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 20:22:56,154 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 20:22:56,154 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 20:22:56,650 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 20:22:59,221 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 20:22:59,223 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 20:22:59,223 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 20:23:00,656 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 20:23:00,656 - config - INFO - [display_ready_banner:38] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 20:23:00,656 - config - INFO - [display_ready_banner:39] -  2 workers ready to process requests
2025-06-23 20:37:01,611 - main - INFO - [lifespan:58] - Shutting down CogView4 API server...
2025-06-23 20:37:01,611 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 20:37:02,405 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 20:37:02,405 - main - INFO - [lifespan:62] - Worker pool shut down successfully
2025-06-23 20:37:05,842 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:37:05,843 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:37:05,843 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:37:05,843 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:37:05,843 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:37:05,843 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:37:05,843 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:37:05,843 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:37:09,454 - __main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:37:09,466 - __main__ - INFO - [<module>:481] - Starting Uvicorn server for CogView4 API...
2025-06-23 20:37:09,486 - main - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:37:09,488 - main - INFO - [lifespan:59] - Starting CogView4 API server...
2025-06-23 20:37:09,488 - main - INFO - [get_worker_pool:48] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 20:37:09,488 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 20:37:09,756 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:37:09,756 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:37:09,756 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:37:09,756 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:37:09,756 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:37:09,756 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:37:09,756 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:37:09,757 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:37:13,294 - __mp_main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:37:13,306 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 20:37:13,308 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 20:37:13,308 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 20:37:13,308 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 20:37:13,308 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 20:37:13,309 - main - INFO - [lifespan:62] - Worker pool initialization process started.
2025-06-23 20:37:13,571 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:37:13,571 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:37:13,571 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:37:13,571 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:37:13,571 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:37:13,571 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:37:13,571 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:37:13,571 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:37:13,571 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:37:13,571 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:37:13,571 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:37:13,571 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:37:13,571 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:37:13,571 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:37:13,571 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:37:13,571 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:37:17,124 - __mp_main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:37:17,129 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 20:37:17,145 - __mp_main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:37:17,150 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 20:37:17,151 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 20:37:17,265 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 20:37:17,265 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 20:37:20,133 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 20:37:20,286 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 20:37:20,287 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 20:37:23,933 - main - INFO - [save_to_gallery:343] - Received save to gallery request
2025-06-23 20:37:23,938 - main - INFO - [save_to_gallery:382] - Original image format: JPEG, size: (400, 400)
2025-06-23 20:37:23,941 - main - INFO - [save_to_gallery:406] - Image resized from 400x400 to 40x40 and saved as JPEG to: static/images/image-1750682243.jpg
2025-06-23 20:37:23,942 - main - INFO - [save_to_gallery:450] - Gallery JSON updated with new image ID: 2
2025-06-23 20:37:50,730 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 20:37:50,732 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 20:37:50,732 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 20:37:51,355 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 20:37:52,751 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 20:37:52,752 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 20:37:52,752 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 20:37:53,358 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 20:37:53,358 - config - INFO - [display_ready_banner:38] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 20:37:53,358 - config - INFO - [display_ready_banner:39] -  2 workers ready to process requests
2025-06-23 20:40:17,597 - main - INFO - [create_image:153] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 20:40:17,598 - main - INFO - [generate_sse_stream:105] - Starting SSE stream generation
2025-06-23 20:40:18,160 - Worker0 - INFO - Worker 0: Processing batched request a12e0d7f with 1 prompts, stream=True
2025-06-23 20:40:27,960 - main - INFO - [create_image:153] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 20:40:27,961 - main - INFO - [generate_sse_stream:105] - Starting SSE stream generation
2025-06-23 20:40:28,473 - Worker1 - INFO - Worker 1: Processing batched request 6e61c030 with 1 prompts, stream=True
2025-06-23 20:40:45,522 - main - INFO - [generate_sse_stream:139] - SSE streaming completed in 17.56s
2025-06-23 20:40:50,865 - main - INFO - [save_to_gallery:343] - Received save to gallery request
2025-06-23 20:40:50,866 - main - INFO - [save_to_gallery:382] - Original image format: JPEG, size: (512, 512)
2025-06-23 20:40:50,871 - main - INFO - [save_to_gallery:406] - Image resized from 512x512 to 51x51 and saved as JPEG to: static/images/image-1750682450.jpg
2025-06-23 20:40:50,871 - main - INFO - [save_to_gallery:450] - Gallery JSON updated with new image ID: 1
2025-06-23 20:40:53,532 - main - INFO - [get_gallery:273] - Received gallery request
2025-06-23 20:40:53,532 - main - INFO - [get_gallery:319] - Loaded 1 images from gallery JSON
2025-06-23 20:41:20,263 - main - INFO - [get_gallery:273] - Received gallery request
2025-06-23 20:41:20,263 - main - INFO - [get_gallery:319] - Loaded 1 images from gallery JSON
2025-06-23 20:47:47,242 - main - INFO - [get_gallery:273] - Received gallery request
2025-06-23 20:47:47,243 - main - INFO - [get_gallery:319] - Loaded 1 images from gallery JSON
2025-06-23 20:47:51,458 - main - INFO - [create_image:153] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 20:47:51,459 - main - INFO - [generate_sse_stream:105] - Starting SSE stream generation
2025-06-23 20:47:52,025 - Worker1 - INFO - Worker 1: Processing batched request db58d5c5 with 1 prompts, stream=True
2025-06-23 20:48:07,826 - main - INFO - [generate_sse_stream:139] - SSE streaming completed in 16.37s
2025-06-23 20:48:23,624 - main - INFO - [lifespan:68] - Shutting down CogView4 API server...
2025-06-23 20:48:23,624 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 20:48:24,489 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 20:48:24,489 - main - INFO - [lifespan:72] - Worker pool shut down successfully
2025-06-23 20:48:27,570 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:48:27,570 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:48:27,570 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:48:27,570 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:48:27,570 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:48:27,570 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:48:27,570 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:48:27,570 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:48:31,177 - __main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:48:31,189 - __main__ - INFO - [<module>:481] - Starting Uvicorn server for CogView4 API...
2025-06-23 20:48:31,208 - main - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:48:31,211 - main - INFO - [lifespan:59] - Starting CogView4 API server...
2025-06-23 20:48:31,211 - main - INFO - [get_worker_pool:48] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 20:48:31,211 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 20:48:31,479 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:48:31,479 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:48:31,479 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:48:31,479 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:48:31,479 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:48:31,479 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:48:31,479 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:48:31,479 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:48:34,990 - __mp_main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:48:35,003 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 20:48:35,006 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 20:48:35,006 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 20:48:35,006 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 20:48:35,007 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 20:48:35,007 - main - INFO - [lifespan:62] - Worker pool initialization process started.
2025-06-23 20:48:35,270 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:48:35,270 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:48:35,270 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:48:35,270 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:48:35,270 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:48:35,270 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:48:35,270 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 20:48:35,270 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:48:35,270 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:48:35,270 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 20:48:35,270 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 20:48:35,270 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 20:48:35,270 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 20:48:35,270 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 20:48:35,270 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 20:48:35,270 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 20:48:38,867 - __mp_main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:48:38,867 - __mp_main__ - INFO - [<module>:96] - Static files mounted successfully
2025-06-23 20:48:38,873 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 20:48:38,873 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 20:48:38,873 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 20:48:38,981 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 20:48:38,981 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 20:48:41,876 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 20:48:42,032 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 20:48:42,032 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 20:49:12,666 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 20:49:12,667 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 20:49:12,667 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 20:49:13,052 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 20:49:14,686 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 20:49:14,688 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 20:49:14,688 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 20:49:15,054 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 20:49:15,055 - config - INFO - [display_ready_banner:38] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 20:49:15,055 - config - INFO - [display_ready_banner:39] -  2 workers ready to process requests
2025-06-23 20:49:33,866 - main - INFO - [create_image:153] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 20:49:33,867 - main - INFO - [generate_sse_stream:105] - Starting SSE stream generation
2025-06-23 20:49:34,400 - Worker0 - INFO - Worker 0: Processing batched request b6314243 with 1 prompts, stream=True
2025-06-23 20:49:50,902 - main - INFO - [generate_sse_stream:139] - SSE streaming completed in 17.03s
2025-06-23 20:50:02,912 - main - INFO - [save_to_gallery:343] - Received save to gallery request
2025-06-23 20:50:02,918 - main - INFO - [save_to_gallery:382] - Original image format: JPEG, size: (512, 512)
2025-06-23 20:50:02,924 - main - INFO - [save_to_gallery:406] - Image resized from 512x512 to 256x256 and saved as JPEG to: static/images/image-1750683002.jpg
2025-06-23 20:50:02,925 - main - INFO - [save_to_gallery:450] - Gallery JSON updated with new image ID: 2
2025-06-23 20:50:05,131 - main - INFO - [get_gallery:273] - Received gallery request
2025-06-23 20:50:05,131 - main - INFO - [get_gallery:319] - Loaded 2 images from gallery JSON
2025-06-23 20:51:38,464 - main - INFO - [get_gallery:273] - Received gallery request
2025-06-23 20:51:38,465 - main - INFO - [get_gallery:319] - Loaded 2 images from gallery JSON
2025-06-23 20:52:08,604 - main - INFO - [get_gallery:273] - Received gallery request
2025-06-23 20:52:08,604 - main - INFO - [get_gallery:319] - Loaded 1 images from gallery JSON
2025-06-23 21:01:45,701 - main - INFO - [lifespan:68] - Shutting down CogView4 API server...
2025-06-23 21:01:45,702 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 21:01:46,486 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 21:01:46,486 - main - INFO - [lifespan:72] - Worker pool shut down successfully
2025-06-23 21:01:49,686 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:01:49,686 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:01:49,686 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:01:49,686 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:01:49,686 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:01:49,686 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:01:49,686 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:01:49,686 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:01:53,354 - __main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:01:53,366 - __main__ - INFO - [<module>:515] - Starting Uvicorn server for CogView4 API...
2025-06-23 21:01:53,386 - main - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:01:53,388 - main - INFO - [lifespan:60] - Starting CogView4 API server...
2025-06-23 21:01:53,389 - main - INFO - [get_worker_pool:49] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 21:01:53,389 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 21:01:53,657 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:01:53,657 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:01:53,657 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:01:53,657 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:01:53,657 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:01:53,657 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:01:53,657 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:01:53,657 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:01:57,180 - __mp_main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:01:57,193 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 21:01:57,196 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 21:01:57,196 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 21:01:57,196 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 21:01:57,197 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 21:01:57,197 - main - INFO - [lifespan:63] - Worker pool initialization process started.
2025-06-23 21:01:57,459 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:01:57,459 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:01:57,459 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:01:57,459 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:01:57,459 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:01:57,459 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:01:57,459 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:01:57,459 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:01:57,459 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:01:57,460 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:01:57,460 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:01:57,460 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:01:57,460 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:01:57,460 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:01:57,460 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:01:57,460 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:02:00,480 - main - INFO - [translate_prompt_api:275] - Received prompt translation request: prompt='A beautiful sunset over a calm ocean with golden w...'
2025-06-23 21:02:01,041 - __mp_main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:02:01,047 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 21:02:01,047 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 21:02:01,050 - __mp_main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:02:01,056 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 21:02:01,153 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 21:02:01,153 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 21:02:01,381 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:02:01,384 - main - INFO - [translate_prompt_api:293] - Prompt translation completed in 0.90s
2025-06-23 21:02:01,388 - main - INFO - [translate_prompt_api:275] - Received prompt translation request: prompt='...'
2025-06-23 21:02:02,078 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:02:02,080 - main - INFO - [translate_prompt_api:293] - Prompt translation completed in 0.69s
2025-06-23 21:02:02,083 - main - INFO - [translate_prompt_api:275] - Received prompt translation request: prompt='A futuristic cityscape with flying cars, neon ligh...'
2025-06-23 21:02:03,155 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:02:03,156 - main - INFO - [translate_prompt_api:293] - Prompt translation completed in 1.07s
2025-06-23 21:02:03,158 - main - INFO - [translate_prompt_api:275] - Received prompt translation request: prompt='...'
2025-06-23 21:02:04,059 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 21:02:04,082 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:02:04,083 - main - INFO - [translate_prompt_api:293] - Prompt translation completed in 0.92s
2025-06-23 21:02:04,086 - main - INFO - [translate_prompt_api:275] - Received prompt translation request: prompt='A beautiful anime girl with blue hair in a magical...'
2025-06-23 21:02:04,211 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 21:02:04,211 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 21:02:05,092 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:02:05,094 - main - INFO - [translate_prompt_api:293] - Prompt translation completed in 1.01s
2025-06-23 21:02:05,101 - main - INFO - [optimize_prompt:234] - Received prompt optimization request: prompt=' A beautiful anime girl...'
2025-06-23 21:02:08,458 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:02:08,461 - main - INFO - [optimize_prompt:252] - Prompt optimization completed in 3.36s
2025-06-23 21:02:35,794 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 21:02:35,795 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 21:02:35,796 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 21:02:37,245 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 21:02:37,281 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 21:02:37,282 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 21:02:37,282 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 21:02:39,248 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 21:02:39,248 - config - INFO - [display_ready_banner:38] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 21:02:39,248 - config - INFO - [display_ready_banner:39] -  2 workers ready to process requests
2025-06-23 21:04:14,203 - main - INFO - [get_gallery:315] - Received gallery request
2025-06-23 21:04:14,203 - main - INFO - [get_gallery:361] - Loaded 1 images from gallery JSON
2025-06-23 21:04:22,072 - main - INFO - [optimize_prompt:234] - Received prompt optimization request: prompt='A beautiful landscape with mountains and a lake at...'
2025-06-23 21:04:25,344 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:04:25,346 - main - INFO - [optimize_prompt:252] - Prompt optimization completed in 3.27s
2025-06-23 21:04:29,226 - main - INFO - [translate_prompt_api:275] - Received prompt translation request: prompt='This image captures a breathtaking landscape at su...'
2025-06-23 21:04:32,617 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:04:32,618 - main - INFO - [translate_prompt_api:293] - Prompt translation completed in 3.39s
2025-06-23 21:04:43,272 - main - INFO - [create_image:154] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 21:04:43,273 - main - INFO - [generate_sse_stream:106] - Starting SSE stream generation
2025-06-23 21:04:43,875 - Worker1 - INFO - Worker 1: Processing batched request ab567b71 with 1 prompts, stream=True
2025-06-23 21:05:00,707 - main - INFO - [generate_sse_stream:140] - SSE streaming completed in 17.43s
2025-06-23 21:05:05,688 - main - INFO - [save_to_gallery:385] - Received save to gallery request
2025-06-23 21:05:05,696 - main - INFO - [save_to_gallery:424] - Original image format: JPEG, size: (512, 512)
2025-06-23 21:05:05,701 - main - INFO - [save_to_gallery:440] - Image saved as JPEG to: static/images/image-1750683905.jpg
2025-06-23 21:05:05,702 - main - INFO - [save_to_gallery:484] - Gallery JSON updated with new image ID: 2
2025-06-23 21:05:07,589 - main - INFO - [get_gallery:315] - Received gallery request
2025-06-23 21:05:07,589 - main - INFO - [get_gallery:361] - Loaded 2 images from gallery JSON
2025-06-23 21:07:25,133 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 21:07:25,134 - main - INFO - [generate_sse_stream:106] - Starting SSE stream generation
2025-06-23 21:07:25,692 - Worker1 - INFO - Worker 1: Processing batched request c505109e with 1 prompts, stream=True
2025-06-23 21:07:41,213 - main - INFO - [generate_sse_stream:140] - SSE streaming completed in 16.08s
2025-06-23 21:08:16,711 - main - INFO - [save_to_gallery:385] - Received save to gallery request
2025-06-23 21:08:16,712 - main - INFO - [save_to_gallery:424] - Original image format: JPEG, size: (512, 512)
2025-06-23 21:08:16,716 - main - INFO - [save_to_gallery:440] - Image saved as JPEG to: static/images/image-1750684096.jpg
2025-06-23 21:08:16,716 - main - INFO - [save_to_gallery:484] - Gallery JSON updated with new image ID: 3
2025-06-23 21:08:19,593 - main - INFO - [get_gallery:315] - Received gallery request
2025-06-23 21:08:19,594 - main - INFO - [get_gallery:361] - Loaded 3 images from gallery JSON
2025-06-23 21:10:36,602 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=False, seed=None
2025-06-23 21:10:37,148 - Worker1 - INFO - Worker 1: Processing batched request 0ba5660c with 1 prompts, stream=False
2025-06-23 21:10:49,684 - main - ERROR - [create_image:188] - Request failed: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 180, in create_image
    images_b64 = result['images']
TypeError: list indices must be integers or slices, not str
2025-06-23 21:10:49,690 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=False, seed=12345
2025-06-23 21:10:50,266 - Worker1 - INFO - Worker 1: Processing batched request 7177fcf2 with 1 prompts, stream=False
2025-06-23 21:11:02,922 - main - ERROR - [create_image:188] - Request failed: list indices must be integers or slices, not str
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 180, in create_image
    images_b64 = result['images']
TypeError: list indices must be integers or slices, not str
2025-06-23 21:11:02,927 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=True, seed=None
2025-06-23 21:11:02,927 - main - INFO - [generate_sse_stream:106] - Starting SSE stream generation
2025-06-23 21:11:03,483 - Worker1 - INFO - Worker 1: Processing batched request 48c6d3f3 with 1 prompts, stream=True
2025-06-23 21:11:19,450 - main - INFO - [generate_sse_stream:140] - SSE streaming completed in 16.52s
2025-06-23 21:11:19,455 - main - INFO - [get_gallery:315] - Received gallery request
2025-06-23 21:11:19,455 - main - INFO - [get_gallery:361] - Loaded 3 images from gallery JSON
2025-06-23 21:12:57,935 - main - INFO - [lifespan:69] - Shutting down CogView4 API server...
2025-06-23 21:12:57,935 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 21:12:58,774 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 21:12:58,774 - main - INFO - [lifespan:73] - Worker pool shut down successfully
2025-06-23 21:13:01,690 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:13:01,690 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:13:01,690 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:13:01,690 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:13:01,690 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:13:01,690 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:13:01,690 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:13:01,690 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:13:05,322 - __main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:13:05,335 - __main__ - INFO - [<module>:515] - Starting Uvicorn server for CogView4 API...
2025-06-23 21:13:05,354 - main - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:13:05,357 - main - INFO - [lifespan:60] - Starting CogView4 API server...
2025-06-23 21:13:05,357 - main - INFO - [get_worker_pool:49] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 21:13:05,357 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 21:13:05,627 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:13:05,627 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:13:05,627 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:13:05,627 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:13:05,627 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:13:05,627 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:13:05,627 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:13:05,627 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:13:09,136 - __mp_main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:13:09,149 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 21:13:09,151 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 21:13:09,152 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 21:13:09,152 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 21:13:09,152 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 21:13:09,152 - main - INFO - [lifespan:63] - Worker pool initialization process started.
2025-06-23 21:13:09,415 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:13:09,415 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:13:09,415 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:13:09,415 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:13:09,415 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:13:09,415 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:13:09,415 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:13:09,415 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:13:09,416 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:13:09,416 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:13:09,416 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:13:09,416 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:13:09,416 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:13:09,416 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:13:09,416 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:13:09,416 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:13:12,965 - __mp_main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:13:12,971 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 21:13:12,977 - __mp_main__ - INFO - [<module>:97] - Static files mounted successfully
2025-06-23 21:13:12,983 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 21:13:12,983 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 21:13:13,102 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 21:13:13,102 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 21:13:15,973 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 21:13:16,104 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 21:13:16,104 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 21:13:24,848 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=False, seed=None
2025-06-23 21:13:46,509 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 21:13:46,511 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 21:13:46,511 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 21:13:46,511 - Worker0 - INFO - Worker 0: Processing batched request 7083cb6a with 1 prompts, stream=False
2025-06-23 21:13:47,203 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-23 21:13:48,399 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 21:13:48,401 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 21:13:48,401 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 21:13:49,207 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 21:13:49,207 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 21:13:49,207 - config - INFO - [display_ready_banner:40] -  2 workers ready to process requests
2025-06-23 21:13:59,976 - main - INFO - [create_image:184] - Non-streaming request completed in 35.13s with seed 2632737165
2025-06-23 21:13:59,989 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=False, seed=12345
2025-06-23 21:14:00,560 - Worker0 - INFO - Worker 0: Processing batched request e918e216 with 1 prompts, stream=False
2025-06-23 21:14:13,292 - main - INFO - [create_image:184] - Non-streaming request completed in 13.30s with seed 12345
2025-06-23 21:14:13,304 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=True, seed=None
2025-06-23 21:14:13,309 - main - INFO - [generate_sse_stream:106] - Starting SSE stream generation
2025-06-23 21:14:13,879 - Worker0 - INFO - Worker 0: Processing batched request b5c35210 with 1 prompts, stream=True
2025-06-23 21:14:29,939 - main - INFO - [generate_sse_stream:140] - SSE streaming completed in 16.63s
2025-06-23 21:14:29,943 - main - INFO - [get_gallery:315] - Received gallery request
2025-06-23 21:14:29,943 - main - INFO - [get_gallery:361] - Loaded 3 images from gallery JSON
2025-06-23 21:15:16,448 - main - INFO - [create_image:154] - Received image generation request: prompt='A beautiful sunset over mountains with golden clou...', stream=False, seed=None
2025-06-23 21:15:16,964 - Worker0 - INFO - Worker 0: Processing batched request 2ee46605 with 1 prompts, stream=False
2025-06-23 21:15:29,566 - main - INFO - [create_image:184] - Non-streaming request completed in 13.12s with seed 2632827578
2025-06-23 21:15:29,579 - main - INFO - [save_to_gallery:385] - Received save to gallery request
2025-06-23 21:15:29,584 - main - INFO - [save_to_gallery:424] - Original image format: PNG, size: (512, 512)
2025-06-23 21:15:30,480 - main - INFO - [save_to_gallery:440] - Image saved as PNG to: static/images/image-1750684529.png
2025-06-23 21:15:30,481 - main - INFO - [save_to_gallery:484] - Gallery JSON updated with new image ID: 4
2025-06-23 21:15:30,485 - main - INFO - [get_gallery:315] - Received gallery request
2025-06-23 21:15:30,485 - main - INFO - [get_gallery:361] - Loaded 4 images from gallery JSON
2025-06-23 21:15:30,487 - main - INFO - [create_image:154] - Received image generation request: prompt='A magical forest with glowing mushrooms and fairy ...', stream=False, seed=None
2025-06-23 21:15:31,084 - Worker1 - INFO - Worker 1: Processing batched request b792d885 with 1 prompts, stream=False
2025-06-23 21:15:44,112 - main - INFO - [create_image:184] - Non-streaming request completed in 13.62s with seed 2632841738
2025-06-23 21:15:44,127 - main - INFO - [save_to_gallery:385] - Received save to gallery request
2025-06-23 21:15:44,128 - main - INFO - [save_to_gallery:424] - Original image format: PNG, size: (512, 512)
2025-06-23 21:15:44,600 - main - INFO - [save_to_gallery:440] - Image saved as PNG to: static/images/image-1750684544.png
2025-06-23 21:15:44,600 - main - INFO - [save_to_gallery:484] - Gallery JSON updated with new image ID: 5
2025-06-23 21:15:44,604 - main - INFO - [get_gallery:315] - Received gallery request
2025-06-23 21:15:44,604 - main - INFO - [get_gallery:361] - Loaded 5 images from gallery JSON
2025-06-23 21:20:54,851 - main - INFO - [lifespan:69] - Shutting down CogView4 API server...
2025-06-23 21:20:54,851 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-23 21:20:55,706 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-23 21:20:55,706 - main - INFO - [lifespan:73] - Worker pool shut down successfully
2025-06-23 21:20:58,774 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:20:58,774 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:20:58,774 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:20:58,774 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:20:58,774 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:20:58,775 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:20:58,775 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:20:58,775 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:21:02,395 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-23 21:21:02,408 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-23 21:21:02,428 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-23 21:21:02,430 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-23 21:21:02,430 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-23 21:21:02,430 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-23 21:21:02,700 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:21:02,700 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:21:02,701 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:21:02,701 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:21:02,701 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:21:02,701 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:21:02,701 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:21:02,701 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:21:06,301 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-23 21:21:06,315 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-23 21:21:06,318 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-23 21:21:06,318 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-23 21:21:06,319 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-23 21:21:06,319 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-23 21:21:06,319 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-23 21:21:06,582 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:21:06,582 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:21:06,582 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:21:06,582 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:21:06,582 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:21:06,582 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:21:06,582 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:21:06,582 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:21:06,582 - config - INFO - [<module>:28] - Log level: INFO
2025-06-23 21:21:06,582 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-23 21:21:06,582 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-23 21:21:06,582 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-23 21:21:06,582 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-23 21:21:06,582 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-23 21:21:06,582 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-23 21:21:06,582 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-23 21:21:10,196 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-23 21:21:10,203 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-23 21:21:10,203 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-23 21:21:10,239 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-23 21:21:10,245 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-23 21:21:10,313 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-23 21:21:10,313 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-23 21:21:13,248 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-23 21:21:13,408 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-23 21:21:13,409 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-23 21:21:15,000 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-23 21:21:15,002 - main - INFO - [get_gallery:362] - Loaded 0 images from gallery JSON
2025-06-23 21:21:23,780 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='A beautiful landscape with mountains and a lake at...'
2025-06-23 21:21:24,785 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:21:24,789 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 1.01s
2025-06-23 21:21:33,047 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 21:21:33,047 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:21:44,045 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-23 21:21:44,047 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-23 21:21:44,047 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-23 21:21:44,047 - Worker0 - INFO - Worker 0: Processing batched request ed8e8fd3 with 1 prompts, stream=True
2025-06-23 21:21:44,365 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-23 21:21:44,367 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-23 21:21:44,367 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-23 21:21:44,369 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-23 21:21:44,369 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-23 21:21:44,369 - config - INFO - [display_ready_banner:40] -  2 workers ready to process requests
2025-06-23 21:22:00,514 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 27.47s
2025-06-23 21:22:06,235 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-23 21:22:06,242 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 512)
2025-06-23 21:22:06,246 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750684926.jpg
2025-06-23 21:22:06,246 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 1
2025-06-23 21:22:11,834 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-23 21:22:11,835 - main - INFO - [get_gallery:362] - Loaded 1 images from gallery JSON
2025-06-23 21:23:03,905 - main - INFO - [optimize_prompt:235] - Received prompt optimization request: prompt='Cute and adorable fluffy cute creature fantasy, dr...'
2025-06-23 21:23:07,382 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:23:07,383 - main - INFO - [optimize_prompt:253] - Prompt optimization completed in 3.48s
2025-06-23 21:23:10,253 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='In this enchanting, dreamlike fantasy scene, a sup...'
2025-06-23 21:23:13,388 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 21:23:13,390 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 3.14s
2025-06-23 21:23:41,900 - main - INFO - [create_image:155] - Received image generation request: prompt='ArtStation...', stream=True, seed=None
2025-06-23 21:23:41,901 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:23:42,446 - Worker0 - INFO - Worker 0: Processing batched request 6c03fe94 with 1 prompts, stream=True
2025-06-23 21:24:08,047 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.15s
2025-06-23 21:26:57,449 - main - INFO - [create_image:155] - Received image generation request: prompt='ArtStation...', stream=True, seed=None
2025-06-23 21:26:57,450 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:26:58,009 - Worker0 - INFO - Worker 0: Processing batched request ef279e72 with 1 prompts, stream=True
2025-06-23 21:27:23,470 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.02s
2025-06-23 21:29:06,745 - main - INFO - [create_image:155] - Received image generation request: prompt='ArtStation...', stream=True, seed=None
2025-06-23 21:29:06,745 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:29:07,283 - Worker1 - INFO - Worker 1: Processing batched request 5183547c with 1 prompts, stream=True
2025-06-23 21:29:24,059 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 17.31s
2025-06-23 21:29:45,763 - main - INFO - [create_image:155] - Received image generation request: prompt='ArtStation...', stream=True, seed=None
2025-06-23 21:29:45,764 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:29:46,338 - Worker0 - INFO - Worker 0: Processing batched request 79bb7891 with 1 prompts, stream=True
2025-06-23 21:30:11,718 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.95s
2025-06-23 21:48:59,664 - main - INFO - [create_image:155] - Received image generation request: prompt='A beautiful sunset over mountains...', stream=False, seed=None
2025-06-23 21:49:00,224 - Worker1 - INFO - Worker 1: Processing batched request bd692af2 with 1 prompts, stream=False
2025-06-23 21:49:12,824 - main - INFO - [create_image:185] - Non-streaming request completed in 13.16s with seed 2634850838
2025-06-23 21:49:12,835 - main - INFO - [create_image:155] - Received image generation request: prompt='ArtStation...', stream=False, seed=12345
2025-06-23 21:49:13,342 - Worker1 - INFO - Worker 1: Processing batched request d8324210 with 1 prompts, stream=False
2025-06-23 21:49:32,540 - main - INFO - [create_image:185] - Non-streaming request completed in 19.70s with seed 12345
2025-06-23 21:49:32,559 - main - INFO - [create_image:155] - Received image generation request: prompt='ArtStation...', stream=True, seed=None
2025-06-23 21:49:32,560 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:49:33,070 - Worker0 - INFO - Worker 0: Processing batched request 92693631 with 1 prompts, stream=True
2025-06-23 21:49:58,364 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.80s
2025-06-23 21:49:58,368 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-23 21:49:58,368 - main - INFO - [get_gallery:362] - Loaded 1 images from gallery JSON
2025-06-23 21:51:36,410 - main - INFO - [create_image:155] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 21:51:36,411 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:51:36,934 - Worker0 - INFO - Worker 0: Processing batched request 2d991523 with 1 prompts, stream=True
2025-06-23 21:52:02,526 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.12s
2025-06-23 21:53:00,493 - main - INFO - [create_image:155] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 21:53:00,494 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:53:01,043 - Worker0 - INFO - Worker 0: Processing batched request 59ee0bd7 with 1 prompts, stream=True
2025-06-23 21:53:26,760 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.27s
2025-06-23 21:58:02,512 - main - INFO - [create_image:155] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 21:58:02,512 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:58:03,027 - Worker1 - INFO - Worker 1: Processing batched request baf846eb with 1 prompts, stream=True
2025-06-23 21:58:28,504 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.99s
2025-06-23 21:59:39,576 - main - INFO - [create_image:155] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-23 21:59:39,577 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 21:59:40,156 - Worker1 - INFO - Worker 1: Processing batched request 0d06a1ca with 1 prompts, stream=True
2025-06-23 22:00:05,783 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.21s
2025-06-23 22:00:19,267 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-23 22:00:19,267 - main - INFO - [get_gallery:362] - Loaded 1 images from gallery JSON
2025-06-23 22:00:47,876 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='Cute and adorable fluffy cute creature fantasy, dr...'
2025-06-23 22:00:48,875 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 22:00:48,877 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 1.00s
2025-06-23 22:00:52,183 - main - INFO - [optimize_prompt:235] - Received prompt optimization request: prompt='Artstation...'
2025-06-23 22:00:55,251 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 22:00:55,253 - main - INFO - [optimize_prompt:253] - Prompt optimization completed in 3.07s
2025-06-23 22:01:03,620 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:01:03,621 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:01:04,165 - Worker1 - INFO - Worker 1: Processing batched request b5a8772a with 1 prompts, stream=True
2025-06-23 22:01:29,239 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.62s
2025-06-23 22:01:34,939 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-23 22:01:34,940 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 912)
2025-06-23 22:01:34,947 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750687294.jpg
2025-06-23 22:01:34,947 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 2
2025-06-23 22:01:36,675 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-23 22:01:36,676 - main - INFO - [get_gallery:362] - Loaded 2 images from gallery JSON
2025-06-23 22:02:18,432 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='A robot holding chalk looking at a blackboard that...'
2025-06-23 22:02:19,987 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 22:02:19,988 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 1.56s
2025-06-23 22:02:28,041 - main - INFO - [optimize_prompt:235] - Received prompt optimization request: prompt='...'
2025-06-23 22:02:32,485 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-23 22:02:32,486 - main - INFO - [optimize_prompt:253] - Prompt optimization completed in 4.45s
2025-06-23 22:02:42,943 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:02:42,944 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:02:43,498 - Worker0 - INFO - Worker 0: Processing batched request dc6565b3 with 1 prompts, stream=True
2025-06-23 22:03:09,434 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.49s
2025-06-23 22:04:26,907 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:04:26,908 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:04:27,437 - Worker0 - INFO - Worker 0: Processing batched request 8dea4f76 with 1 prompts, stream=True
2025-06-23 22:04:53,329 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.42s
2025-06-23 22:05:20,912 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:05:20,913 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:05:21,510 - Worker0 - INFO - Worker 0: Processing batched request 764619d5 with 1 prompts, stream=True
2025-06-23 22:05:47,179 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.27s
2025-06-23 22:06:00,210 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:06:00,211 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:06:00,761 - Worker0 - INFO - Worker 0: Processing batched request cf82e356 with 1 prompts, stream=True
2025-06-23 22:06:27,275 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 27.06s
2025-06-23 22:06:37,476 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:06:37,476 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:06:38,010 - Worker0 - INFO - Worker 0: Processing batched request 3a1038f0 with 1 prompts, stream=True
2025-06-23 22:07:03,474 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.00s
2025-06-23 22:07:40,834 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:07:40,835 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:07:41,395 - Worker1 - INFO - Worker 1: Processing batched request 0060bd00 with 1 prompts, stream=True
2025-06-23 22:08:06,719 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.88s
2025-06-23 22:08:16,318 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:08:16,319 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:08:16,840 - Worker0 - INFO - Worker 0: Processing batched request 2018a1c9 with 1 prompts, stream=True
2025-06-23 22:08:43,060 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.74s
2025-06-23 22:09:19,586 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-23 22:09:19,587 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-23 22:09:20,122 - Worker1 - INFO - Worker 1: Processing batched request 6a4b0607 with 1 prompts, stream=True
2025-06-23 22:09:58,173 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 38.59s
2025-06-24 10:03:52,979 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 10:03:52,980 - main - INFO - [get_gallery:362] - Loaded 2 images from gallery JSON
2025-06-24 10:04:01,258 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=2635574777
2025-06-24 10:04:01,259 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:04:01,763 - Worker0 - INFO - Worker 0: Processing batched request 17bb3001 with 1 prompts, stream=True
2025-06-24 10:04:26,889 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.63s
2025-06-24 10:04:56,701 - main - INFO - [optimize_prompt:235] - Received prompt optimization request: prompt='short light skinned black female, long curly with ...'
2025-06-24 10:04:59,806 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 10:04:59,810 - main - INFO - [optimize_prompt:253] - Prompt optimization completed in 3.11s
2025-06-24 10:05:02,022 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='In this captivating portrait, a short, light-skinn...'
2025-06-24 10:05:05,176 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 10:05:05,178 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 3.16s
2025-06-24 10:05:19,834 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=2635574777
2025-06-24 10:05:19,836 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:05:20,360 - Worker1 - INFO - Worker 1: Processing batched request 27778d32 with 1 prompts, stream=True
2025-06-24 10:05:45,683 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.85s
2025-06-24 10:06:09,928 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=2635574777
2025-06-24 10:06:09,929 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:06:10,521 - Worker1 - INFO - Worker 1: Processing batched request d4313e95 with 1 prompts, stream=True
2025-06-24 10:06:35,940 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.01s
2025-06-24 10:06:58,081 - main - INFO - [create_image:155] - Received image generation request: prompt='short light skinned female, long curly with no fri...', stream=True, seed=2635574777
2025-06-24 10:06:58,082 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:06:58,680 - Worker1 - INFO - Worker 1: Processing batched request 35622b83 with 1 prompts, stream=True
2025-06-24 10:07:23,913 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.83s
2025-06-24 10:07:34,144 - main - INFO - [create_image:155] - Received image generation request: prompt='short light skinned female, long curly with no fri...', stream=True, seed=763898619
2025-06-24 10:07:34,145 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:07:34,726 - Worker1 - INFO - Worker 1: Processing batched request 606d451f with 1 prompts, stream=True
2025-06-24 10:08:00,115 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.97s
2025-06-24 10:08:25,939 - main - INFO - [create_image:155] - Received image generation request: prompt='short light skinned female, long curly with no fri...', stream=True, seed=None
2025-06-24 10:08:25,940 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:08:26,490 - Worker0 - INFO - Worker 0: Processing batched request 6889931e with 1 prompts, stream=True
2025-06-24 10:08:51,854 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.91s
2025-06-24 10:09:06,571 - main - INFO - [create_image:155] - Received image generation request: prompt='short light skinned female, long curly with no fri...', stream=True, seed=None
2025-06-24 10:09:06,572 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:09:07,140 - Worker1 - INFO - Worker 1: Processing batched request 030f5343 with 1 prompts, stream=True
2025-06-24 10:09:22,489 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 15.92s
2025-06-24 10:09:41,689 - main - INFO - [optimize_prompt:235] - Received prompt optimization request: prompt='short light skinned female, long curly with no fri...'
2025-06-24 10:09:44,772 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 10:09:44,774 - main - INFO - [optimize_prompt:253] - Prompt optimization completed in 3.09s
2025-06-24 10:09:49,137 - main - INFO - [create_image:155] - Received image generation request: prompt='In this captivating image, a short, light-skinned ...', stream=True, seed=None
2025-06-24 10:09:49,138 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:09:49,693 - Worker0 - INFO - Worker 0: Processing batched request 22d314ad with 1 prompts, stream=True
2025-06-24 10:10:05,504 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.37s
2025-06-24 10:13:08,996 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='In this captivating image, a short, light-skinned ...'
2025-06-24 10:13:12,214 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 10:13:12,217 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 3.22s
2025-06-24 10:54:18,158 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 10:54:18,158 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 10:54:19,058 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 10:54:19,058 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 10:54:22,723 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:54:22,723 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:54:22,723 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:54:22,723 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:54:22,723 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:54:22,723 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:54:22,723 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:54:22,723 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:54:26,372 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:54:26,385 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 10:54:26,404 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:54:26,407 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 10:54:26,407 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 10:54:26,407 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 10:54:26,676 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:54:26,676 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:54:26,676 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:54:26,677 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:54:26,677 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:54:26,677 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:54:26,677 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:54:26,677 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:54:30,176 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:54:30,189 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-24 10:54:30,191 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-24 10:54:30,192 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 10:54:30,192 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 10:54:30,192 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 10:54:30,193 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 10:54:30,455 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:54:30,455 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:54:30,455 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:54:30,455 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:54:30,455 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:54:30,455 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:54:30,455 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:54:30,455 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:54:30,456 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:54:30,456 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:54:30,456 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:54:30,456 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:54:30,456 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:54:30,456 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:54:30,456 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:54:30,456 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:54:34,034 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:54:34,034 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:54:34,039 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 10:54:34,040 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 10:54:34,040 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-24 10:54:34,150 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 10:54:34,151 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 10:54:37,043 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-24 10:54:37,155 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-24 10:54:37,155 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-24 10:55:26,464 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-24 10:55:26,466 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-24 10:55:26,466 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-24 10:55:27,375 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 10:55:27,377 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 10:55:27,377 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 10:55:28,260 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 10:55:28,260 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 10:55:28,260 - config - INFO - [display_ready_banner:40] -  2 workers ready to process requests
2025-06-24 10:55:33,233 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 10:55:33,237 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:55:33,796 - Worker0 - INFO - Worker 0: Processing batched request eda3ae0d with 1 prompts, stream=True
2025-06-24 10:55:34,609 - Worker0 - ERROR - Worker 0: Error in batched streaming request: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/worker.py", line 230, in process_batched_streaming_request
    pipeline(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 602, in __call__
    timesteps, num_inference_steps = retrieve_timesteps(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 110, in retrieve_timesteps
    scheduler.set_timesteps(timesteps=timesteps, sigmas=sigmas, device=device, **kwargs)
TypeError: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
2025-06-24 10:55:34,610 - main - ERROR - [generate_sse_stream:145] - SSE stream error: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 121, in generate_sse_stream
    async for result in pool.submit_request(stream=True, **request_params):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 192, in submit_request
    async for result in self._stream_results(request_id):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 235, in _stream_results
    raise Exception(result.data['error'])
Exception: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
2025-06-24 10:58:40,513 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 10:58:40,513 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 10:58:41,319 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 10:58:41,319 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 10:58:44,229 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:58:44,229 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:58:44,229 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:58:44,229 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:58:44,229 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:58:44,229 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:58:44,229 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:58:44,229 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:58:47,845 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:58:47,858 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 10:58:47,874 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:58:47,877 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 10:58:47,877 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 10:58:47,877 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 10:58:48,145 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:58:48,145 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:58:48,145 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:58:48,145 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:58:48,145 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:58:48,145 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:58:48,145 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:58:48,145 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:58:51,678 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:58:51,690 - processing - INFO - [__init__:126] - Initializing worker pool with 2 workers
2025-06-24 10:58:51,693 - processing - INFO - [__init__:135] - Worker pool initialized with 2 workers
2025-06-24 10:58:51,693 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 10:58:51,693 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 10:58:51,694 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 10:58:51,694 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 10:58:51,956 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:58:51,956 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 10:58:51,956 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:58:51,956 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:58:51,956 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 10:58:51,956 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:58:51,956 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 10:58:51,956 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:58:51,956 - config - INFO - [<module>:31] - Number of worker processes: 2
2025-06-24 10:58:51,956 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:58:51,956 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 10:58:51,956 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:58:51,956 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 10:58:51,956 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:58:51,956 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 10:58:51,956 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 10:58:55,518 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:58:55,524 - Worker1 - INFO - Worker 1 starting up with proper logging configured...
2025-06-24 10:58:55,538 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 10:58:55,544 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 10:58:55,544 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 10:58:55,662 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 10:58:55,662 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 10:58:58,527 - Worker1 - INFO - Worker 1: Using cuda:1 (GPU 1 of 8 available)
2025-06-24 10:58:58,682 - Worker1 - INFO - Worker 1: CUDA device 1 initialized successfully
2025-06-24 10:58:58,682 - Worker1 - INFO - Worker 1: Loading model on cuda:1...
2025-06-24 10:59:28,487 - Worker1 - INFO - Worker 1: Successfully loaded base pipeline on cuda:1
2025-06-24 10:59:28,489 - Worker1 - INFO - Worker 1: Model loading status updated to True
2025-06-24 10:59:28,489 - Worker1 - INFO - Worker 1 ready to process requests on cuda:1
2025-06-24 10:59:29,739 - processing - INFO - [_monitor_worker_readiness:288] - Worker loading progress: 1/2 workers ready
2025-06-24 10:59:30,033 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 10:59:30,035 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 10:59:30,035 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 10:59:31,741 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 10:59:31,742 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 10:59:31,742 - config - INFO - [display_ready_banner:40] -  2 workers ready to process requests
2025-06-24 10:59:56,762 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 10:59:56,766 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 10:59:57,296 - Worker1 - INFO - Worker 1: Processing batched request ceafbba3 with 1 prompts, stream=True
2025-06-24 10:59:58,108 - Worker1 - ERROR - Worker 1: Error in batched streaming request: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/worker.py", line 230, in process_batched_streaming_request
    pipeline(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 602, in __call__
    timesteps, num_inference_steps = retrieve_timesteps(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 110, in retrieve_timesteps
    scheduler.set_timesteps(timesteps=timesteps, sigmas=sigmas, device=device, **kwargs)
TypeError: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
2025-06-24 10:59:58,120 - main - ERROR - [generate_sse_stream:145] - SSE stream error: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 121, in generate_sse_stream
    async for result in pool.submit_request(stream=True, **request_params):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 192, in submit_request
    async for result in self._stream_results(request_id):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 235, in _stream_results
    raise Exception(result.data['error'])
Exception: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
2025-06-24 11:14:11,805 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 11:14:11,805 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 11:14:12,654 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 11:14:12,654 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 11:19:22,363 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:19:22,364 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:19:22,364 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:19:22,364 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:19:22,364 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:19:22,364 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:19:22,364 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:19:22,364 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:19:28,663 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:19:28,691 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 11:19:28,753 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:19:28,758 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 11:19:28,759 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 11:19:28,759 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 11:19:30,084 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:19:30,085 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:19:30,085 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:19:30,085 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:19:30,085 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:19:30,085 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:19:30,085 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:19:30,085 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:19:36,299 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:19:36,329 - processing - INFO - [__init__:126] - Initializing worker pool with 1 workers
2025-06-24 11:19:36,332 - processing - INFO - [__init__:135] - Worker pool initialized with 1 workers
2025-06-24 11:19:36,334 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 11:19:36,335 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 11:19:36,335 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 11:19:36,336 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 11:19:37,633 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:19:37,634 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:19:37,634 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:19:37,634 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:19:37,634 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:19:37,634 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:19:37,634 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:19:37,634 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:19:43,812 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:19:43,824 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 11:19:43,824 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 11:19:43,940 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 11:19:43,940 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 11:20:06,811 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 11:20:06,815 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 11:20:06,815 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 11:20:08,385 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 11:20:08,386 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 11:20:08,386 - config - INFO - [display_ready_banner:40] -  1 workers ready to process requests
2025-06-24 11:20:32,990 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:20:32,997 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:20:33,535 - Worker0 - INFO - Worker 0: Processing batched request 58675158 with 1 prompts, stream=True
2025-06-24 11:23:18,893 - Worker0 - ERROR - Worker 0: Error in batched streaming request: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/worker.py", line 230, in process_batched_streaming_request
    pipeline(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 602, in __call__
    timesteps, num_inference_steps = retrieve_timesteps(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 110, in retrieve_timesteps
    scheduler.set_timesteps(timesteps=timesteps, sigmas=sigmas, device=device, **kwargs)
TypeError: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
2025-06-24 11:23:18,896 - main - ERROR - [generate_sse_stream:145] - SSE stream error: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 121, in generate_sse_stream
    async for result in pool.submit_request(stream=True, **request_params):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 192, in submit_request
    async for result in self._stream_results(request_id):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 235, in _stream_results
    raise Exception(result.data['error'])
Exception: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'sigmas'
2025-06-24 11:29:19,021 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 11:29:19,021 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 11:29:20,369 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 11:29:20,369 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 11:29:37,769 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:29:37,769 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:29:37,769 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:29:37,769 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:29:37,769 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:29:37,769 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:29:37,769 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:29:37,769 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:29:41,385 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:29:41,398 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 11:29:41,414 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:29:41,417 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 11:29:41,417 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 11:29:41,417 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 11:29:41,687 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:29:41,687 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:29:41,687 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:29:41,687 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:29:41,687 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:29:41,687 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:29:41,687 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:29:41,687 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:29:45,163 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:29:45,177 - processing - INFO - [__init__:126] - Initializing worker pool with 1 workers
2025-06-24 11:29:45,178 - processing - INFO - [__init__:135] - Worker pool initialized with 1 workers
2025-06-24 11:29:45,179 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 11:29:45,179 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 11:29:45,179 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 11:29:45,180 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 11:29:45,442 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:29:45,442 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:29:45,442 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:29:45,442 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:29:45,442 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:29:45,442 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:29:45,443 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:29:45,443 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:29:48,968 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:29:48,974 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 11:29:48,974 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 11:29:49,090 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 11:29:49,090 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 11:30:12,510 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 11:30:12,511 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 11:30:12,511 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 11:30:13,212 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 11:30:13,212 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 11:30:13,212 - config - INFO - [display_ready_banner:40] -  1 workers ready to process requests
2025-06-24 11:30:25,971 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:30:25,975 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:30:26,543 - Worker0 - INFO - Worker 0: Processing batched request 9daf6073 with 1 prompts, stream=True
2025-06-24 11:30:27,479 - Worker0 - ERROR - Worker 0: Error in batched streaming request: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'mu'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/worker.py", line 230, in process_batched_streaming_request
    pipeline(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 614, in __call__
    timesteps, num_inference_steps = retrieve_timesteps(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 112, in retrieve_timesteps
    scheduler.set_timesteps(**kwargs_to_pass)
TypeError: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'mu'
2025-06-24 11:30:27,480 - main - ERROR - [generate_sse_stream:145] - SSE stream error: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'mu'
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 121, in generate_sse_stream
    async for result in pool.submit_request(stream=True, **request_params):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 192, in submit_request
    async for result in self._stream_results(request_id):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 235, in _stream_results
    raise Exception(result.data['error'])
Exception: HeunDiscreteScheduler.set_timesteps() got an unexpected keyword argument 'mu'
2025-06-24 11:34:53,918 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 11:34:53,919 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 11:34:54,546 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 11:34:54,546 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 11:34:57,890 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:34:57,890 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:34:57,890 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:34:57,890 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:34:57,890 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:34:57,891 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:34:57,891 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:34:57,891 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:35:01,511 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:35:01,523 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 11:35:01,540 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:35:01,542 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 11:35:01,542 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 11:35:01,543 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 11:35:01,811 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:35:01,812 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:35:01,812 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:35:01,812 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:35:01,812 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:35:01,812 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:35:01,812 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:35:01,812 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:35:05,348 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:35:05,361 - processing - INFO - [__init__:126] - Initializing worker pool with 1 workers
2025-06-24 11:35:05,363 - processing - INFO - [__init__:135] - Worker pool initialized with 1 workers
2025-06-24 11:35:05,363 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 11:35:05,363 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 11:35:05,364 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 11:35:05,364 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 11:35:05,630 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:35:05,630 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:35:05,630 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:35:05,630 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:35:05,630 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:35:05,630 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:35:05,630 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:35:05,630 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:35:09,161 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:35:09,167 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 11:35:09,167 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 11:35:09,281 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 11:35:09,281 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 11:35:33,435 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 11:35:33,437 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 11:35:33,437 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 11:35:35,403 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 11:35:35,403 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 11:35:35,403 - config - INFO - [display_ready_banner:40] -  1 workers ready to process requests
2025-06-24 11:35:40,580 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:35:40,585 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:35:41,126 - Worker0 - INFO - Worker 0: Processing batched request ae030da0 with 1 prompts, stream=True
2025-06-24 11:35:41,929 - Worker0 - ERROR - Worker 0: Error in batched streaming request: Can only pass one of `num_inference_steps` or `custom_timesteps`.
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/worker.py", line 230, in process_batched_streaming_request
    pipeline(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 625, in __call__
    timesteps, num_inference_steps = retrieve_timesteps(
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/pipelines/cogview4/pipeline_cogview4.py", line 123, in retrieve_timesteps
    scheduler.set_timesteps(**kwargs_to_pass)
  File "/root/myprojects/CogView4-FastAPI/.venv/lib/python3.10/site-packages/diffusers/schedulers/scheduling_heun_discrete.py", line 289, in set_timesteps
    raise ValueError("Can only pass one of `num_inference_steps` or `custom_timesteps`.")
ValueError: Can only pass one of `num_inference_steps` or `custom_timesteps`.
2025-06-24 11:35:41,938 - main - ERROR - [generate_sse_stream:145] - SSE stream error: Can only pass one of `num_inference_steps` or `custom_timesteps`.
Traceback (most recent call last):
  File "/root/myprojects/CogView4-FastAPI/main.py", line 121, in generate_sse_stream
    async for result in pool.submit_request(stream=True, **request_params):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 192, in submit_request
    async for result in self._stream_results(request_id):
  File "/root/myprojects/CogView4-FastAPI/processing.py", line 235, in _stream_results
    raise Exception(result.data['error'])
Exception: Can only pass one of `num_inference_steps` or `custom_timesteps`.
2025-06-24 11:39:39,032 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 11:39:39,032 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 11:39:39,612 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 11:39:39,612 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 11:41:29,967 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:41:29,968 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:41:29,968 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:41:29,968 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:41:29,968 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:41:29,968 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:41:29,968 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:41:29,968 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:41:33,679 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:41:33,692 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 11:41:33,709 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:41:33,711 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 11:41:33,711 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 11:41:33,711 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 11:41:33,982 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:41:33,982 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:41:33,982 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:41:33,982 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:41:33,982 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:41:33,982 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:41:33,982 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:41:33,982 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:41:37,560 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:41:37,574 - processing - INFO - [__init__:126] - Initializing worker pool with 1 workers
2025-06-24 11:41:37,575 - processing - INFO - [__init__:135] - Worker pool initialized with 1 workers
2025-06-24 11:41:37,576 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 11:41:37,576 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 11:41:37,577 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 11:41:37,577 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 11:41:37,840 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:41:37,840 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:41:37,840 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:41:37,840 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:41:37,840 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:41:37,840 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:41:37,840 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:41:37,840 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:41:41,377 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:41:41,382 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 11:41:41,382 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 11:41:41,493 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 11:41:41,494 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 11:42:05,598 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 11:42:05,599 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 11:42:05,599 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 11:42:05,613 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 11:42:05,614 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 11:42:05,614 - config - INFO - [display_ready_banner:40] -  1 workers ready to process requests
2025-06-24 11:42:17,035 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:42:17,040 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:42:17,543 - Worker0 - INFO - Worker 0: Processing batched request 48d54459 with 1 prompts, stream=True
2025-06-24 11:42:48,069 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 31.03s
2025-06-24 11:45:30,856 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 11:45:30,856 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 11:45:31,610 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 11:45:31,611 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 11:45:34,764 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:45:34,764 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:45:34,764 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:45:34,764 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:45:34,764 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:45:34,764 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:45:34,764 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:45:34,764 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:45:38,411 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:45:38,423 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 11:45:38,440 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:45:38,443 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 11:45:38,443 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 11:45:38,443 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 11:45:38,714 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:45:38,714 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:45:38,714 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:45:38,714 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:45:38,714 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:45:38,714 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:45:38,714 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:45:38,714 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:45:42,246 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:45:42,258 - processing - INFO - [__init__:126] - Initializing worker pool with 1 workers
2025-06-24 11:45:42,259 - processing - INFO - [__init__:135] - Worker pool initialized with 1 workers
2025-06-24 11:45:42,260 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 11:45:42,260 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 11:45:42,260 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 11:45:42,261 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 11:45:42,523 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:45:42,523 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:45:42,524 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:45:42,524 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:45:42,524 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:45:42,524 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:45:42,524 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:45:42,524 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:45:46,057 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:45:46,063 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 11:45:46,063 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 11:45:46,171 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 11:45:46,171 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 11:46:10,343 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 11:46:10,344 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 11:46:10,344 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 11:46:12,301 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 11:46:12,301 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 11:46:12,301 - config - INFO - [display_ready_banner:40] -  1 workers ready to process requests
2025-06-24 11:46:28,638 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:46:28,639 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:46:29,141 - Worker0 - INFO - Worker 0: Processing batched request 169ae541 with 1 prompts, stream=True
2025-06-24 11:46:59,577 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 30.94s
2025-06-24 11:48:08,826 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 11:48:08,827 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 11:48:09,546 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 11:48:09,546 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 11:48:12,297 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:48:12,297 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:48:12,297 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:48:12,297 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:48:12,297 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:48:12,297 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:48:12,297 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:48:12,297 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:48:15,928 - __main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:48:15,941 - __main__ - INFO - [<module>:521] - Starting Uvicorn server for CogView4 API...
2025-06-24 11:48:15,958 - main - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:48:15,960 - main - INFO - [lifespan:61] - Starting CogView4 API server...
2025-06-24 11:48:15,960 - main - INFO - [get_worker_pool:50] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 11:48:15,960 - processing - INFO - [__init__:112] - Set multiprocessing start method to 'spawn'
2025-06-24 11:48:16,229 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:48:16,229 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:48:16,229 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:48:16,229 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:48:16,229 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:48:16,229 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:48:16,229 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:48:16,230 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:48:19,733 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:48:19,746 - processing - INFO - [__init__:126] - Initializing worker pool with 1 workers
2025-06-24 11:48:19,748 - processing - INFO - [__init__:135] - Worker pool initialized with 1 workers
2025-06-24 11:48:19,748 - processing - INFO - [__init__:142] - Prompt batching enabled - timeout checker started
2025-06-24 11:48:19,749 - processing - INFO - [_monitor_worker_readiness:275] - Starting worker readiness monitoring...
2025-06-24 11:48:19,749 - processing - INFO - [__init__:149] - Worker readiness monitor started
2025-06-24 11:48:19,749 - main - INFO - [lifespan:64] - Worker pool initialization process started.
2025-06-24 11:48:20,012 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 11:48:20,012 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 11:48:20,012 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 11:48:20,012 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 11:48:20,012 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 11:48:20,012 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 11:48:20,012 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 11:48:20,012 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 11:48:23,523 - __mp_main__ - INFO - [<module>:98] - Static files mounted successfully
2025-06-24 11:48:23,529 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 11:48:23,529 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 11:48:23,638 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 11:48:23,638 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 11:48:47,739 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 11:48:47,741 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 11:48:47,741 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 11:48:47,789 - processing - INFO - [_monitor_worker_readiness:282] - All workers have loaded models - displaying ready banner
2025-06-24 11:48:47,789 - config - INFO - [display_ready_banner:39] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 11:48:47,789 - config - INFO - [display_ready_banner:40] -  1 workers ready to process requests
2025-06-24 11:48:57,998 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:48:58,003 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:48:58,511 - Worker0 - INFO - Worker 0: Processing batched request 88f3e6b9 with 1 prompts, stream=True
2025-06-24 11:49:15,134 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 17.13s
2025-06-24 11:49:45,482 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:49:45,483 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:49:46,075 - Worker0 - INFO - Worker 0: Processing batched request 9f5180f6 with 1 prompts, stream=True
2025-06-24 11:50:01,916 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.43s
2025-06-24 11:50:09,190 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 11:50:09,195 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 512)
2025-06-24 11:50:09,199 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750737009.jpg
2025-06-24 11:50:09,199 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 3
2025-06-24 11:50:11,572 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 11:50:11,573 - main - INFO - [get_gallery:362] - Loaded 3 images from gallery JSON
2025-06-24 11:51:32,467 - main - INFO - [create_image:155] - Received image generation request: prompt='A minion holding a sign that says 'GPUStack'. The ...', stream=True, seed=None
2025-06-24 11:51:32,468 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:51:33,032 - Worker0 - INFO - Worker 0: Processing batched request d63e9169 with 1 prompts, stream=True
2025-06-24 11:51:48,917 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.45s
2025-06-24 11:52:01,351 - main - INFO - [create_image:155] - Received image generation request: prompt='A minion holding a sign that says 'AiLinks'. The b...', stream=True, seed=None
2025-06-24 11:52:01,351 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:52:01,871 - Worker0 - INFO - Worker 0: Processing batched request e93e32c6 with 1 prompts, stream=True
2025-06-24 11:52:17,675 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.32s
2025-06-24 11:52:23,932 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 11:52:23,933 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 512)
2025-06-24 11:52:23,939 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750737143.jpg
2025-06-24 11:52:23,939 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 4
2025-06-24 11:52:25,822 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 11:52:25,823 - main - INFO - [get_gallery:362] - Loaded 4 images from gallery JSON
2025-06-24 11:53:37,001 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='short light skinned white female, long curly with ...'
2025-06-24 11:53:38,490 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 11:53:38,493 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 1.49s
2025-06-24 11:53:55,612 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:53:55,613 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:53:56,157 - Worker0 - INFO - Worker 0: Processing batched request 53eb53a9 with 1 prompts, stream=True
2025-06-24 11:54:21,370 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.76s
2025-06-24 11:54:45,784 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 11:54:45,784 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:54:46,325 - Worker0 - INFO - Worker 0: Processing batched request 1fdbd059 with 1 prompts, stream=True
2025-06-24 11:55:11,619 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.83s
2025-06-24 11:55:19,289 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 11:55:19,290 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 912)
2025-06-24 11:55:19,297 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750737319.jpg
2025-06-24 11:55:19,298 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 5
2025-06-24 11:55:20,677 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 11:55:20,678 - main - INFO - [get_gallery:362] - Loaded 5 images from gallery JSON
2025-06-24 11:56:37,613 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 11:56:37,614 - main - INFO - [get_gallery:362] - Loaded 5 images from gallery JSON
2025-06-24 11:56:59,825 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 11:56:59,826 - main - INFO - [get_gallery:362] - Loaded 5 images from gallery JSON
2025-06-24 11:57:06,037 - main - INFO - [create_image:155] - Received image generation request: prompt='building contain 5 floors with blue glass curtain ...', stream=True, seed=None
2025-06-24 11:57:06,038 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:57:06,608 - Worker0 - INFO - Worker 0: Processing batched request 31222499 with 1 prompts, stream=True
2025-06-24 11:57:22,248 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.21s
2025-06-24 11:58:01,848 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='building contain 5 floors with blue glass curtain ...'
2025-06-24 11:58:02,876 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 11:58:02,878 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 1.03s
2025-06-24 11:58:17,550 - main - INFO - [create_image:155] - Received image generation request: prompt='5 ...', stream=True, seed=None
2025-06-24 11:58:17,551 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:58:18,102 - Worker0 - INFO - Worker 0: Processing batched request a5cfd2c8 with 1 prompts, stream=True
2025-06-24 11:58:33,644 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.09s
2025-06-24 11:58:53,620 - main - INFO - [create_image:155] - Received image generation request: prompt='5 ...', stream=True, seed=None
2025-06-24 11:58:53,621 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 11:58:54,149 - Worker0 - INFO - Worker 0: Processing batched request 20c0d420 with 1 prompts, stream=True
2025-06-24 11:59:09,672 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.05s
2025-06-24 12:00:11,682 - main - INFO - [create_image:155] - Received image generation request: prompt='Building Global Partner Ecosystems for Scalable Gr...', stream=True, seed=None
2025-06-24 12:00:11,683 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:00:12,255 - Worker0 - INFO - Worker 0: Processing batched request 04e0f919 with 1 prompts, stream=True
2025-06-24 12:00:27,881 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.20s
2025-06-24 12:01:09,428 - main - INFO - [create_image:155] - Received image generation request: prompt='building, pool, friend, cat...', stream=True, seed=None
2025-06-24 12:01:09,428 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:01:10,028 - Worker0 - INFO - Worker 0: Processing batched request e4b4926a with 1 prompts, stream=True
2025-06-24 12:01:25,605 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.18s
2025-06-24 12:01:47,793 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 12:01:47,794 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 512)
2025-06-24 12:01:47,797 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750737707.jpg
2025-06-24 12:01:47,798 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 6
2025-06-24 12:01:49,855 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:01:49,856 - main - INFO - [get_gallery:362] - Loaded 6 images from gallery JSON
2025-06-24 12:02:42,643 - main - INFO - [create_image:155] - Received image generation request: prompt='A young woman lies on a vibrant blue fluffy surfac...', stream=True, seed=None
2025-06-24 12:02:42,644 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:02:43,151 - Worker0 - INFO - Worker 0: Processing batched request f36d5eea with 1 prompts, stream=True
2025-06-24 12:02:58,864 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.22s
2025-06-24 12:03:09,811 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='A young woman lies on a vibrant blue fluffy surfac...'
2025-06-24 12:03:11,783 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:03:11,784 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 1.97s
2025-06-24 12:03:21,452 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=2686073765
2025-06-24 12:03:21,453 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:03:22,001 - Worker0 - INFO - Worker 0: Processing batched request fed003d3 with 1 prompts, stream=True
2025-06-24 12:03:37,748 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.29s
2025-06-24 12:03:45,731 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 12:03:45,732 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 512)
2025-06-24 12:03:45,737 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750737825.jpg
2025-06-24 12:03:45,737 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 7
2025-06-24 12:03:47,703 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:03:47,703 - main - INFO - [get_gallery:362] - Loaded 7 images from gallery JSON
2025-06-24 12:05:45,567 - main - INFO - [create_image:155] - Received image generation request: prompt='1girl, cat_ears, solo, blush, sfw, stuffed animal,...', stream=True, seed=None
2025-06-24 12:05:45,568 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:05:46,079 - Worker0 - INFO - Worker 0: Processing batched request b217a95f with 1 prompts, stream=True
2025-06-24 12:06:35,121 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 49.55s
2025-06-24 12:06:43,077 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='1girl, cat_ears, solo, blush, sfw, stuffed animal,...'
2025-06-24 12:06:45,018 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:06:45,020 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 1.94s
2025-06-24 12:07:17,971 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='1 girl, cat ears, solo, blush, safe for work, plus...'
2025-06-24 12:07:21,593 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:07:21,594 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 3.62s
2025-06-24 12:08:40,637 - main - INFO - [create_image:155] - Received image generation request: prompt='1...', stream=True, seed=None
2025-06-24 12:08:40,638 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:08:41,193 - Worker0 - INFO - Worker 0: Processing batched request 3576e893 with 1 prompts, stream=True
2025-06-24 12:09:02,724 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:09:02,725 - main - INFO - [get_gallery:362] - Loaded 7 images from gallery JSON
2025-06-24 12:09:30,398 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 49.76s
2025-06-24 12:10:12,571 - main - INFO - [create_image:155] - Received image generation request: prompt='1...', stream=True, seed=None
2025-06-24 12:10:12,572 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:10:13,106 - Worker0 - INFO - Worker 0: Processing batched request bf339f78 with 1 prompts, stream=True
2025-06-24 12:11:24,532 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 71.96s
2025-06-24 12:11:39,859 - main - INFO - [create_image:155] - Received image generation request: prompt='1...', stream=True, seed=None
2025-06-24 12:11:39,860 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:11:40,413 - Worker0 - INFO - Worker 0: Processing batched request ce8787ea with 1 prompts, stream=True
2025-06-24 12:12:52,980 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 73.12s
2025-06-24 12:13:01,040 - main - INFO - [create_image:155] - Received image generation request: prompt='1...', stream=True, seed=None
2025-06-24 12:13:01,041 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:13:01,614 - Worker0 - INFO - Worker 0: Processing batched request bacb3c28 with 1 prompts, stream=True
2025-06-24 12:14:14,823 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 73.78s
2025-06-24 12:14:51,385 - main - INFO - [create_image:155] - Received image generation request: prompt='1...', stream=True, seed=None
2025-06-24 12:14:51,386 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:14:51,951 - Worker0 - INFO - Worker 0: Processing batched request 9c7a76a9 with 1 prompts, stream=True
2025-06-24 12:16:04,828 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 73.44s
2025-06-24 12:16:20,488 - main - INFO - [create_image:155] - Received image generation request: prompt='1...', stream=True, seed=None
2025-06-24 12:16:20,488 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:16:21,062 - Worker0 - INFO - Worker 0: Processing batched request 919c6b6d with 1 prompts, stream=True
2025-06-24 12:17:34,102 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 73.61s
2025-06-24 12:17:39,649 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 12:17:39,651 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 910)
2025-06-24 12:17:39,657 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750738659.jpg
2025-06-24 12:17:39,658 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 8
2025-06-24 12:17:45,461 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:17:45,462 - main - INFO - [get_gallery:362] - Loaded 8 images from gallery JSON
2025-06-24 12:20:36,385 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:20:36,386 - main - INFO - [get_gallery:362] - Loaded 8 images from gallery JSON
2025-06-24 12:30:05,803 - main - INFO - [create_image:155] - Received image generation request: prompt='This is a high-contrast, monochromatic photograph ...', stream=True, seed=None
2025-06-24 12:30:05,804 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:30:06,360 - Worker0 - INFO - Worker 0: Processing batched request 868da342 with 1 prompts, stream=True
2025-06-24 12:30:32,043 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.24s
2025-06-24 12:30:37,187 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='This is a high-contrast, monochromatic photograph ...'
2025-06-24 12:30:41,907 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:30:41,909 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 4.72s
2025-06-24 12:31:13,300 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 12:31:13,301 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:31:13,851 - Worker0 - INFO - Worker 0: Processing batched request 633baaa7 with 1 prompts, stream=True
2025-06-24 12:31:39,722 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.42s
2025-06-24 12:32:09,891 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 12:32:09,891 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:32:10,422 - Worker0 - INFO - Worker 0: Processing batched request fa93576d with 1 prompts, stream=True
2025-06-24 12:32:36,333 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.44s
2025-06-24 12:32:50,175 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 12:32:50,175 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:32:50,773 - Worker0 - INFO - Worker 0: Processing batched request 9acb7e4f with 1 prompts, stream=True
2025-06-24 12:33:17,054 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.88s
2025-06-24 12:33:23,949 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 12:33:23,950 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 287)
2025-06-24 12:33:23,953 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750739603.jpg
2025-06-24 12:33:23,954 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 9
2025-06-24 12:33:31,033 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:33:31,034 - main - INFO - [get_gallery:362] - Loaded 9 images from gallery JSON
2025-06-24 12:35:56,740 - main - INFO - [create_image:155] - Received image generation request: prompt='buildings in the city, neo-Egyptian architecture, ...', stream=True, seed=None
2025-06-24 12:35:56,741 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:35:57,301 - Worker0 - INFO - Worker 0: Processing batched request 06a65f23 with 1 prompts, stream=True
2025-06-24 12:36:22,427 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.69s
2025-06-24 12:36:41,603 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='buildings in the city, neo-Egyptian architecture, ...'
2025-06-24 12:36:42,522 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:36:42,525 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 0.92s
2025-06-24 12:36:49,967 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 12:36:49,968 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:36:50,470 - Worker0 - INFO - Worker 0: Processing batched request c6fef467 with 1 prompts, stream=True
2025-06-24 12:37:15,625 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 25.66s
2025-06-24 12:37:21,777 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 12:37:21,778 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 912)
2025-06-24 12:37:21,786 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750739841.jpg
2025-06-24 12:37:21,786 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 10
2025-06-24 12:37:40,707 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:37:40,708 - main - INFO - [get_gallery:362] - Loaded 10 images from gallery JSON
2025-06-24 12:39:03,737 - main - INFO - [optimize_prompt:235] - Received prompt optimization request: prompt='...'
2025-06-24 12:39:07,299 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:39:07,302 - main - INFO - [optimize_prompt:253] - Prompt optimization completed in 3.57s
2025-06-24 12:39:31,577 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=None
2025-06-24 12:39:31,578 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:39:32,167 - Worker0 - INFO - Worker 0: Processing batched request 8baa83ee with 1 prompts, stream=True
2025-06-24 12:39:57,709 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.13s
2025-06-24 12:40:06,430 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 12:40:06,431 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 912)
2025-06-24 12:40:06,437 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750740006.jpg
2025-06-24 12:40:06,438 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 11
2025-06-24 12:40:16,482 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:40:16,482 - main - INFO - [get_gallery:362] - Loaded 11 images from gallery JSON
2025-06-24 12:41:09,937 - main - INFO - [create_image:155] - Received image generation request: prompt='A beautiful landscape with mountains and a lake at...', stream=True, seed=None
2025-06-24 12:41:09,938 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:41:10,495 - Worker0 - INFO - Worker 0: Processing batched request 96d3809e with 1 prompts, stream=True
2025-06-24 12:41:36,396 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.46s
2025-06-24 12:42:21,164 - main - INFO - [create_image:155] - Received image generation request: prompt='A futuristic city with flying cars...', stream=True, seed=None
2025-06-24 12:42:21,165 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:42:21,691 - Worker0 - INFO - Worker 0: Processing batched request ffffa5ed with 1 prompts, stream=True
2025-06-24 12:42:47,385 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 26.22s
2025-06-24 12:43:08,072 - main - INFO - [save_to_gallery:386] - Received save to gallery request
2025-06-24 12:43:08,073 - main - INFO - [save_to_gallery:430] - Original image format: JPEG, size: (512, 512)
2025-06-24 12:43:08,078 - main - INFO - [save_to_gallery:446] - Image saved as JPEG to: static/images/image-1750740188.jpg
2025-06-24 12:43:08,078 - main - INFO - [save_to_gallery:490] - Gallery JSON updated with new image ID: 12
2025-06-24 12:43:12,360 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:43:12,361 - main - INFO - [get_gallery:362] - Loaded 12 images from gallery JSON
2025-06-24 12:43:23,740 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:43:23,741 - main - INFO - [get_gallery:362] - Loaded 12 images from gallery JSON
2025-06-24 12:43:33,882 - main - INFO - [create_image:155] - Received image generation request: prompt='A futuristic city with flying cars...', stream=True, seed=2688452305
2025-06-24 12:43:33,883 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:43:34,390 - Worker0 - INFO - Worker 0: Processing batched request a07afb0b with 1 prompts, stream=True
2025-06-24 12:43:41,011 - main - INFO - [optimize_prompt:235] - Received prompt optimization request: prompt='A futuristic city with flying cars...'
2025-06-24 12:43:44,231 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:43:44,233 - main - INFO - [optimize_prompt:253] - Prompt optimization completed in 3.22s
2025-06-24 12:43:50,018 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.13s
2025-06-24 12:43:58,139 - main - INFO - [create_image:155] - Received image generation request: prompt='In this vivid, futuristic cityscape, towering skys...', stream=True, seed=2688452305
2025-06-24 12:43:58,139 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:43:58,723 - Worker0 - INFO - Worker 0: Processing batched request 7f270c2e with 1 prompts, stream=True
2025-06-24 12:44:14,512 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.37s
2025-06-24 12:44:22,074 - main - INFO - [translate_prompt_api:276] - Received prompt translation request: prompt='In this vivid, futuristic cityscape, towering skys...'
2025-06-24 12:44:24,862 - httpx - INFO - [_send_single_request:1025] - HTTP Request: POST https://models.dev.ai-links.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-24 12:44:24,863 - main - INFO - [translate_prompt_api:294] - Prompt translation completed in 2.79s
2025-06-24 12:44:53,357 - main - INFO - [create_image:155] - Received image generation request: prompt='...', stream=True, seed=2688452305
2025-06-24 12:44:53,358 - main - INFO - [generate_sse_stream:107] - Starting SSE stream generation
2025-06-24 12:44:53,898 - Worker0 - INFO - Worker 0: Processing batched request 95fb7061 with 1 prompts, stream=True
2025-06-24 12:45:05,407 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:45:05,408 - main - INFO - [get_gallery:362] - Loaded 12 images from gallery JSON
2025-06-24 12:45:09,611 - main - INFO - [generate_sse_stream:141] - SSE streaming completed in 16.25s
2025-06-24 12:54:49,710 - main - INFO - [get_gallery:316] - Received gallery request
2025-06-24 12:54:49,711 - main - INFO - [get_gallery:362] - Loaded 12 images from gallery JSON
2025-06-24 13:08:28,015 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:08:28,015 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:08:28,015 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:08:28,015 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:08:28,015 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:08:28,015 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:08:28,015 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:08:28,015 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:08:36,359 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:08:36,359 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:08:36,359 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:08:36,359 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:08:36,359 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:08:36,359 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:08:36,359 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:08:36,359 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:08:39,901 - main - INFO - [<module>:109] - Static files mounted successfully
2025-06-24 13:08:48,008 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:08:48,008 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:08:48,008 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:08:48,009 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:08:48,009 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:08:48,009 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:08:48,009 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:08:48,009 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:08:51,546 - main - INFO - [<module>:109] - Static files mounted successfully
2025-06-24 13:08:51,575 - main - INFO - [lifespan:72] - Starting CogView4 API server...
2025-06-24 13:08:51,576 - main - INFO - [get_worker_pool:61] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 13:08:51,576 - processing - INFO - [__init__:120] - Set multiprocessing start method to 'spawn'
2025-06-24 13:08:51,845 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:08:51,845 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:08:51,845 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:08:51,846 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:08:51,846 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:08:51,846 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:08:51,846 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:08:51,846 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:08:55,385 - main - INFO - [<module>:109] - Static files mounted successfully
2025-06-24 13:08:55,407 - processing - INFO - [__init__:134] - Initializing worker pool with 1 workers
2025-06-24 13:08:55,409 - processing - INFO - [__init__:143] - Worker pool initialized with 1 workers
2025-06-24 13:08:55,410 - processing - INFO - [__init__:150] - Prompt batching enabled - timeout checker started
2025-06-24 13:08:55,410 - processing - INFO - [_monitor_worker_readiness:283] - Starting worker readiness monitoring...
2025-06-24 13:08:55,410 - processing - INFO - [__init__:157] - Worker readiness monitor started
2025-06-24 13:08:55,410 - main - INFO - [lifespan:75] - Worker pool initialization process started.
2025-06-24 13:08:55,411 - main - INFO - [lifespan:81] - Shutting down CogView4 API server...
2025-06-24 13:08:55,411 - processing - INFO - [shutdown:267] - Shutting down worker pool...
2025-06-24 13:08:55,674 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:08:55,674 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:08:55,674 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:08:55,674 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:08:55,674 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:08:55,674 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:08:55,674 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:08:55,674 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:08:59,215 - main - INFO - [<module>:109] - Static files mounted successfully
2025-06-24 13:08:59,231 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 13:08:59,231 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 13:08:59,333 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 13:08:59,333 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 13:09:05,421 - processing - WARNING - [shutdown:277] - Force terminating worker 0
2025-06-24 13:09:06,334 - processing - INFO - [shutdown:280] - Worker pool shutdown complete
2025-06-24 13:09:06,334 - main - INFO - [lifespan:85] - Worker pool shut down successfully
2025-06-24 13:09:32,523 - main - INFO - [lifespan:70] - Shutting down CogView4 API server...
2025-06-24 13:09:32,523 - processing - INFO - [shutdown:259] - Shutting down worker pool...
2025-06-24 13:09:33,276 - processing - INFO - [shutdown:272] - Worker pool shutdown complete
2025-06-24 13:09:33,276 - main - INFO - [lifespan:74] - Worker pool shut down successfully
2025-06-24 13:10:28,847 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:10:28,847 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:10:28,847 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:10:28,847 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:10:28,847 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:10:28,847 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:10:28,847 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:10:28,847 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:10:32,474 - main - INFO - [<module>:109] - Static files mounted successfully
2025-06-24 13:10:32,503 - main - INFO - [lifespan:72] - Starting CogView4 API server...
2025-06-24 13:10:32,503 - main - INFO - [get_worker_pool:61] - Initializing worker pool with model: /gm-models/CogView4-6B
2025-06-24 13:10:32,504 - processing - INFO - [__init__:120] - Set multiprocessing start method to 'spawn'
2025-06-24 13:10:32,773 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:10:32,773 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:10:32,773 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:10:32,773 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:10:32,773 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:10:32,773 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:10:32,773 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:10:32,773 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:10:36,302 - main - INFO - [<module>:109] - Static files mounted successfully
2025-06-24 13:10:36,325 - processing - INFO - [__init__:134] - Initializing worker pool with 1 workers
2025-06-24 13:10:36,327 - processing - INFO - [__init__:143] - Worker pool initialized with 1 workers
2025-06-24 13:10:36,328 - processing - INFO - [__init__:150] - Prompt batching enabled - timeout checker started
2025-06-24 13:10:36,328 - processing - INFO - [_monitor_worker_readiness:283] - Starting worker readiness monitoring...
2025-06-24 13:10:36,328 - processing - INFO - [__init__:157] - Worker readiness monitor started
2025-06-24 13:10:36,328 - main - INFO - [lifespan:75] - Worker pool initialization process started.
2025-06-24 13:10:36,593 - config - INFO - [<module>:28] - Log level: INFO
2025-06-24 13:10:36,593 - config - INFO - [<module>:29] - Log file: cogview4_api.log
2025-06-24 13:10:36,593 - config - INFO - [<module>:30] - Model path: /gm-models/CogView4-6B
2025-06-24 13:10:36,593 - config - INFO - [<module>:31] - Number of worker processes: 1
2025-06-24 13:10:36,593 - config - INFO - [<module>:32] - VRAM protection: Maximum total pixels per request: 4,194,304
2025-06-24 13:10:36,593 - config - INFO - [<module>:33] - Prompt batching enabled: True
2025-06-24 13:10:36,593 - config - INFO - [<module>:35] - Batch timeout: 0.5s
2025-06-24 13:10:36,593 - config - INFO - [<module>:36] - Max batch size: 8
2025-06-24 13:10:40,126 - main - INFO - [<module>:109] - Static files mounted successfully
2025-06-24 13:10:40,141 - Worker0 - INFO - Worker 0 starting up with proper logging configured...
2025-06-24 13:10:40,142 - Worker0 - INFO - Worker 0: Using cuda:0 (GPU 0 of 8 available)
2025-06-24 13:10:40,251 - Worker0 - INFO - Worker 0: CUDA device 0 initialized successfully
2025-06-24 13:10:40,251 - Worker0 - INFO - Worker 0: Loading model on cuda:0...
2025-06-24 13:11:05,035 - Worker0 - INFO - Worker 0: Successfully loaded base pipeline on cuda:0
2025-06-24 13:11:05,036 - Worker0 - INFO - Worker 0: Model loading status updated to True
2025-06-24 13:11:05,036 - Worker0 - INFO - Worker 0 ready to process requests on cuda:0
2025-06-24 13:11:06,370 - processing - INFO - [_monitor_worker_readiness:290] - All workers have loaded models - displaying ready banner
2025-06-24 13:11:06,370 - config - INFO - [display_ready_banner:43] -  CogView4 API Server is fully ready - all workers loaded!
2025-06-24 13:11:06,371 - config - INFO - [display_ready_banner:44] -  1 workers ready to process requests
